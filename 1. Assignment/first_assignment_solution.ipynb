{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #1: Pseudonymisation Techniques and Considerations\n",
    "- Dataset: Crossfit [Daset](https://data.world/bgadoci/crossfit-data) (In this assignment only the athletes file was used) \n",
    "- Credits: Dataset was put together by Sam Swift\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd0b2ac563f828dd"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be93d85a454a3b2d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:34:43.312057Z",
     "start_time": "2024-01-09T12:34:42.324017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id           name               region          team  \\\n",
      "0      2554.0      Pj Ablang           South West   Double Edge   \n",
      "1      3517.0  Derek Abdella                  NaN           NaN   \n",
      "2      4691.0            NaN                  NaN           NaN   \n",
      "3      5164.0    Abo Brandon  Southern California  LAX CrossFit   \n",
      "\n",
      "              affiliate gender   age  height  weight   fran  ...  deadlift  \\\n",
      "0  Double Edge CrossFit   Male  24.0   166.0    70.0    NaN  ...     400.0   \n",
      "1                   NaN   Male  42.0   190.0    70.0    NaN  ...       NaN   \n",
      "2                   NaN    NaN   NaN     NaN     NaN    NaN  ...       NaN   \n",
      "3          LAX CrossFit   Male  40.0     NaN    67.0  211.0  ...     375.0   \n",
      "\n",
      "   backsq  pullups                                   eat  \\\n",
      "0   305.0      NaN                                   NaN   \n",
      "1     NaN      NaN                                   NaN   \n",
      "2     NaN      NaN                                   NaN   \n",
      "3   325.0     25.0  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "2                                         NaN        NaN                 NaN  \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN  \n",
      "\n",
      "[4 rows x 28 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/4kctgn5n2378x2bnhkqv4xpc0000gn/T/ipykernel_3104/3265770945.py:4: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(\"athletes.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read csv as dataframe\n",
    "dataframe = pd.read_csv(\"athletes.csv\")\n",
    "\n",
    "#dataframe[\"test\"] = dataframe[\"height\"]\n",
    "#dataframe[\"height\"] = dataframe[\"weight\"]\n",
    "#dataframe[\"weight\"] = dataframe[\"test\"]\n",
    "\n",
    "#dataframe = dataframe.drop(columns=['test'])\n",
    "\n",
    "#dataframe.to_csv('athletes_new.csv', index=False)\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "## 3.1 Pseudonymisation\n",
    "Goals:\n",
    "-  Determine which attributes qualify as explicit personally identifiable information (3.1.1)\n",
    "    - Why? \n",
    "    - What method was used to identify these? \n",
    "- Generate pseudonymous values for the identified attributes (3.1.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b64615a1710ba2a"
  },
  {
   "cell_type": "markdown",
   "id": "a09a389f01e7c40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1.1 Identifying Attributes containing Personally Identifiable Information (PII)\n",
    "- In order to identify th attributes first a better understanding of the structure of the dataset needs to be obtained\n",
    "    - What columns does the dataset contain and in what format are the attribute values?\n",
    "        - Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 'athlete_id', Example Data: 2554.0\n",
      "Column: 'name', Example Data: Pj Ablang\n",
      "Column: 'region', Example Data: South West\n",
      "Column: 'team', Example Data: Double Edge\n",
      "Column: 'affiliate', Example Data: Double Edge CrossFit\n",
      "Column: 'gender', Example Data: Male\n",
      "Column: 'age', Example Data: 24.0\n",
      "Column: 'height', Example Data: 166.0\n",
      "Column: 'weight', Example Data: 70.0\n",
      "Column: 'fran', Example Data: 211.0\n",
      "Column: 'helen', Example Data: 645.0\n",
      "Column: 'grace', Example Data: 300.0\n",
      "Column: 'filthy50', Example Data: 1053.0\n",
      "Column: 'fgonebad', Example Data: 0.0\n",
      "Column: 'run400', Example Data: 61.0\n",
      "Column: 'run5k', Example Data: 1081.0\n",
      "Column: 'candj', Example Data: 220.0\n",
      "Column: 'snatch', Example Data: 200.0\n",
      "Column: 'deadlift', Example Data: 400.0\n",
      "Column: 'backsq', Example Data: 305.0\n",
      "Column: 'pullups', Example Data: 25.0\n",
      "Column: 'eat', Example Data: I eat 1-3 full cheat meals per week|\n",
      "Column: 'train', Example Data: I workout mostly at a CrossFit Affiliate|I have a coach who determines my programming|I record my workouts|\n",
      "Column: 'background', Example Data: I played youth or high school level sports|I regularly play recreational sports|\n",
      "Column: 'experience', Example Data: I began CrossFit with a coach (e.g. at an affiliate)|I have attended one or more specialty courses|I have had a life changing experience due to CrossFit|\n",
      "Column: 'schedule', Example Data: I do multiple workouts in a day 2x a week|\n",
      "Column: 'howlong', Example Data: 4+ years|\n",
      "Column: 'retrieved_datetime', Example Data: 2015-03-01 22:49:18\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_not_empty_value(df_column):\n",
    "    return df_column.dropna().iloc[0] if not df_column.dropna().empty else None\n",
    "\n",
    "# Iterate each column \n",
    "for column in dataframe.columns:\n",
    "    first_value = get_first_not_not_empty_value(dataframe[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:34:43.596460Z",
     "start_time": "2024-01-09T12:34:43.312183Z"
    }
   },
   "id": "3669191d576f3aae"
  },
  {
   "cell_type": "markdown",
   "id": "6ed2670d051f539a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- By inspecting the different columns and the data format, several attributes which have the potential to contain explicit personally identifiable information can be identified\n",
    "    - `athelete_id`\n",
    "        -  This really depends on the usage of this id! Considerations to take into account are: \n",
    "            - Is the `athlete_id` only used as an internal id of this dataset or does it maybe even refer to an official id?\n",
    "            - Are there other datasets available which may have a similar source to this dataset? Thus, these other datasets may use the same `athlete_id`\n",
    "    - `name`\n",
    "        - The name allows to identify an individual\n",
    "    - `team`\n",
    "        - Depending on the size of the team, this could allow to identify a specific athlete\n",
    "    - `affiliate` \n",
    "        - Depending on the affiliate and the amount of contracted athletes, this could allow to identify an individual\n",
    "    - All stats of the athletes\n",
    "        - If an athlete has really remarkable stats (maybe even a world record in a category), this could allow to identify the individual\n",
    "    - `train` \n",
    "        - If an athlete has a special and famous training routine, this could allow to identify him\n",
    "    - `background`\n",
    "        - If an athlete has a famous background or mentions names, this could allow to identify him\n",
    "    - `experience`\n",
    "        - If an athlete mentions concrete information about his experience (e.g. name of current coach), this could allow to identify him\n",
    "\n",
    "-> As can be seen, all columns could potentially contain outliers which could be then used to identify an individual. As agreed on, for each of the columns (only those holding numbers or names) potentially leaking PII one anonymization technique was chosen which keeps the data loss as minimal as possible without leaking obvious PII. It is important to note, that there could still be columns containing PII. However, this chance for outliers was accepted in this assignment. Columns containing descriptions (e.g. train, background and experience) where to anonymized and thus the chance for outliers accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e7f8a58b909df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1.2 Pseudonymizing\n",
    "- For pseudonymisation especially the name is suitable, because it can be easily replaced with a fake name, without being as obvious or destroying the purpose of the dataset\n",
    "- With the help of the `anonymizedf` library new fake names can be created for the `name` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1f75b29dbfdecd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:34:52.804258Z",
     "start_time": "2024-01-09T12:34:43.589424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id               region          team             affiliate gender  \\\n",
      "0      2554.0           South West   Double Edge  Double Edge CrossFit   Male   \n",
      "1      3517.0                  NaN           NaN                   NaN   Male   \n",
      "2      4691.0                  NaN           NaN                   NaN    NaN   \n",
      "3      5164.0  Southern California  LAX CrossFit          LAX CrossFit   Male   \n",
      "\n",
      "    age  height  weight   fran  helen  ...  backsq  pullups  \\\n",
      "0  24.0   166.0    70.0    NaN    NaN  ...   305.0      NaN   \n",
      "1  42.0   190.0    70.0    NaN    NaN  ...     NaN      NaN   \n",
      "2   NaN     NaN     NaN    NaN    NaN  ...     NaN      NaN   \n",
      "3  40.0     NaN    67.0  211.0  645.0  ...   325.0     25.0   \n",
      "\n",
      "                                    eat  \\\n",
      "0                                   NaN   \n",
      "1                                   NaN   \n",
      "2                                   NaN   \n",
      "3  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \\\n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN   \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN   \n",
      "2                                         NaN        NaN                 NaN   \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN   \n",
      "\n",
      "             name  \n",
      "0    Brian Gibson  \n",
      "1  Timothy Hilton  \n",
      "2    Mark Edwards  \n",
      "3     Annette May  \n",
      "\n",
      "[4 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "from anonymizedf import anonymizedf\n",
    "\n",
    "# Prepare the data to be anonymized\n",
    "an = anonymizedf.anonymize(dataframe)\n",
    "\n",
    "# Add new column with fake name\n",
    "an.fake_names(\"name\")\n",
    "\n",
    "# Drop old original name column\n",
    "dataframe = dataframe.drop(columns=['name'])\n",
    "\n",
    "# Rename Fake_name column in name\n",
    "dataframe = dataframe.rename(columns={'Fake_name': 'name'})\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d638b8091b0fee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 Randomisation\n",
    "Goal:\n",
    "- Use randomisation technique to generate random strings (no meaning) (3.2.1)\n",
    "- Use randomisation technique to generate random but meaningful replacements and create lookup table (3.2.2)\n",
    "\n",
    "### 3.2.1 Random Strings\n",
    "1. Identify columns to replace with random strings\n",
    "    - Here columns containing words can be used \n",
    "        - `name`\n",
    "        - `region`\n",
    "        - `team`\n",
    "        - `affiliate` \n",
    "2. Generate a function which is responsible for generating random values for replacement\n",
    "3. Overwrite each value in respective attributes by using the generated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea49738582c6a6e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:35:00.758015Z",
     "start_time": "2024-01-09T12:34:52.803737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id           region                  team  \\\n",
      "0      2554.0  cBrGejFduFBvfdo  QXGgtKjEwcQWtezpZBdG   \n",
      "1      3517.0  LqGXKeTtWoWcRPZ  MMGXBBibJoEmKYnJCddi   \n",
      "2      4691.0  oyEXpxSWYaDysvI  QqIvecQySchSIrcpQuSZ   \n",
      "3      5164.0  kywVlkZDWChaHUv  zQwPuvWXHnaAPkdpHFuZ   \n",
      "\n",
      "                   affiliate gender   age  height  weight   fran  helen  ...  \\\n",
      "0  NySIYvZIrymIWPOCyDaIkFerl   Male  24.0   166.0    70.0    NaN    NaN  ...   \n",
      "1  wtOyrtDYjEOnynDDqpQXcHUXj   Male  42.0   190.0    70.0    NaN    NaN  ...   \n",
      "2  GZRODWXlzlpTmrdWaPwfmslnM    NaN   NaN     NaN     NaN    NaN    NaN  ...   \n",
      "3  JZuQopwXPwFGWUNFonZBiRtGf   Male  40.0     NaN    67.0  211.0  645.0  ...   \n",
      "\n",
      "   backsq  pullups                                   eat  \\\n",
      "0   305.0      NaN                                   NaN   \n",
      "1     NaN      NaN                                   NaN   \n",
      "2     NaN      NaN                                   NaN   \n",
      "3   325.0     25.0  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \\\n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN   \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN   \n",
      "2                                         NaN        NaN                 NaN   \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN   \n",
      "\n",
      "         name  \n",
      "0  QrkEYuBpfh  \n",
      "1  PhVAAjWlWx  \n",
      "2  OQSiDCqBMm  \n",
      "3  GYXdsTpCZs  \n",
      "\n",
      "[4 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Define function to create random string\n",
    "def generate_random_string(length):\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "# Apply randomization on specified columns\n",
    "dataframe['name'] = dataframe['name'].apply(lambda x: generate_random_string(10))\n",
    "dataframe['region'] = dataframe['region'].apply(lambda x: generate_random_string(15))\n",
    "dataframe['team'] = dataframe['team'].apply(lambda x: generate_random_string(20))\n",
    "dataframe['affiliate'] = dataframe['affiliate'].apply(lambda x: generate_random_string(25))\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72fe81893e96ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2.2 Randomization with meaningful strings\n",
    "1. Identify columns suitable\n",
    "    - name, region, team, affiliate\n",
    "        -  For randomizing the name, the first letter will be kept equal to the original in order to keep similarities\n",
    "2. Write functionality to randomize the values\n",
    "- Interchange Values in Columns Name, Region, Team, Affiliate with meaningful strings to randomize and print out the lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/4kctgn5n2378x2bnhkqv4xpc0000gn/T/ipykernel_3104/2479564682.py:2: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(\"athletes.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id             name       region                          team  \\\n",
      "0      2554.0  Antti JÃ¤rvinen   North East                           NaN   \n",
      "1      3517.0              NaN   North East              CrossFit Decoded   \n",
      "2      4691.0              NaN  Canada West  CrossFit Fire City Singapore   \n",
      "3      5164.0              NaN          NaN                           NaN   \n",
      "\n",
      "                affiliate gender   age  height  weight   fran  ...  deadlift  \\\n",
      "0          CrossFit Baruk   Male  24.0   166.0    70.0    NaN  ...     400.0   \n",
      "1          CrossFit Lakás   Male  42.0   190.0    70.0    NaN  ...       NaN   \n",
      "2                     NaN    NaN   NaN     NaN     NaN    NaN  ...       NaN   \n",
      "3  CrossFit Sherwood Park   Male  40.0     NaN    67.0  211.0  ...     375.0   \n",
      "\n",
      "   backsq  pullups                                   eat  \\\n",
      "0   305.0      NaN                                   NaN   \n",
      "1     NaN      NaN                                   NaN   \n",
      "2     NaN      NaN                                   NaN   \n",
      "3   325.0     25.0  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "2                                         NaN        NaN                 NaN  \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN  \n",
      "\n",
      "[4 rows x 28 columns]\n",
      "            name               region          team             affiliate  \\\n",
      "0      Pj Ablang           South West   Double Edge  Double Edge CrossFit   \n",
      "1  Derek Abdella                  NaN           NaN                   NaN   \n",
      "2            NaN                  NaN           NaN                   NaN   \n",
      "3    Abo Brandon  Southern California  LAX CrossFit          LAX CrossFit   \n",
      "\n",
      "   randomized_name randomized_region               randomized_team  \\\n",
      "0  Antti JÃ¤rvinen        North East                           NaN   \n",
      "1              NaN        North East              CrossFit Decoded   \n",
      "2              NaN       Canada West  CrossFit Fire City Singapore   \n",
      "3              NaN               NaN                           NaN   \n",
      "\n",
      "     randomized_affiliate  \n",
      "0          CrossFit Baruk  \n",
      "1          CrossFit Lakás  \n",
      "2                     NaN  \n",
      "3  CrossFit Sherwood Park  \n"
     ]
    }
   ],
   "source": [
    "# To see the changes properly we have to reload the initial data\n",
    "dataframe = pd.read_csv(\"athletes.csv\")\n",
    "\n",
    "# Copy the original values in mapping dataframe\n",
    "dataframe_mapping = dataframe[['name', 'region', 'team', 'affiliate']].copy()\n",
    "\n",
    "# Interchange values for region, team and affiliate\n",
    "def interchange_values(column_name):\n",
    "    dataframe[column_name] = dataframe[column_name].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Randomize name, region, team and affiliate\n",
    "interchange_values('name')\n",
    "interchange_values('region')\n",
    "interchange_values('team')\n",
    "interchange_values('affiliate')\n",
    "\n",
    "# Copy the modified values in mapping dataframe\n",
    "dataframe_mapping[['randomized_name', 'randomized_region', 'randomized_team', 'randomized_affiliate']] = dataframe[['name', 'region', 'team', 'affiliate']].copy()\n",
    "\n",
    "# Save mapping to csv\n",
    "dataframe_mapping.to_csv('athletes_mapping.csv', index=False)\n",
    "\n",
    "print(dataframe.iloc[:4])\n",
    "print(dataframe_mapping.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:35:02.311485Z",
     "start_time": "2024-01-09T12:35:00.760247Z"
    }
   },
   "id": "fe9526ff779442bc"
  },
  {
   "cell_type": "markdown",
   "id": "c59dc5a07acdb83d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 Aggregation\n",
    "Goal:\n",
    "- Determine attributes which qualify for aggregation (3.3.1)\n",
    "- Write a function to perform that aggregation process (3.3.2)\n",
    "\n",
    "### 3.3.1 Determine Attributes for Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The script iterates through each column in the DataFrame and prints the column name along with the first non-empty value in that column:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42c548ad3a5e556c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfc0b43c8b43a28",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:35:02.645706Z",
     "start_time": "2024-01-09T12:35:02.300665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 'athlete_id', Example Data: 2554.0\n",
      "Column: 'name', Example Data: Antti JÃ¤rvinen\n",
      "Column: 'region', Example Data: North East\n",
      "Column: 'team', Example Data: CrossFit Decoded\n",
      "Column: 'affiliate', Example Data: CrossFit Baruk\n",
      "Column: 'gender', Example Data: Male\n",
      "Column: 'age', Example Data: 24.0\n",
      "Column: 'height', Example Data: 166.0\n",
      "Column: 'weight', Example Data: 70.0\n",
      "Column: 'fran', Example Data: 211.0\n",
      "Column: 'helen', Example Data: 645.0\n",
      "Column: 'grace', Example Data: 300.0\n",
      "Column: 'filthy50', Example Data: 1053.0\n",
      "Column: 'fgonebad', Example Data: 0.0\n",
      "Column: 'run400', Example Data: 61.0\n",
      "Column: 'run5k', Example Data: 1081.0\n",
      "Column: 'candj', Example Data: 220.0\n",
      "Column: 'snatch', Example Data: 200.0\n",
      "Column: 'deadlift', Example Data: 400.0\n",
      "Column: 'backsq', Example Data: 305.0\n",
      "Column: 'pullups', Example Data: 25.0\n",
      "Column: 'eat', Example Data: I eat 1-3 full cheat meals per week|\n",
      "Column: 'train', Example Data: I workout mostly at a CrossFit Affiliate|I have a coach who determines my programming|I record my workouts|\n",
      "Column: 'background', Example Data: I played youth or high school level sports|I regularly play recreational sports|\n",
      "Column: 'experience', Example Data: I began CrossFit with a coach (e.g. at an affiliate)|I have attended one or more specialty courses|I have had a life changing experience due to CrossFit|\n",
      "Column: 'schedule', Example Data: I do multiple workouts in a day 2x a week|\n",
      "Column: 'howlong', Example Data: 4+ years|\n",
      "Column: 'retrieved_datetime', Example Data: 2015-03-01 22:49:18\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_not_empty_value(df_column):\n",
    "    return df_column.dropna().iloc[0] if not df_column.dropna().empty else None\n",
    "\n",
    "# Print each column with its first not empty value\n",
    "for column in dataframe.columns:\n",
    "    first_value = get_first_not_not_empty_value(dataframe[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c31ee64889069",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Numerical attributes can be easily aggregated\n",
    "- Especially those related to information about the individual are suitable for aggregation\n",
    "    - `age`\n",
    "    - `height` \n",
    "    - `weight`\n",
    "- To be able to identify different levels of values, the extremes have to be identified "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The script prints the minimum and maximum values for 'age,' 'height,' and 'weight' columns:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5826be06ba973a42"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabc635f8ab91128",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:35:02.712490Z",
     "start_time": "2024-01-09T12:35:02.637985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum age': 7257      13.0\n",
      "276479    13.0\n",
      "331209    13.0\n",
      "347070    13.0\n",
      "267865    14.0\n",
      "Name: age, dtype: float64, Maximum age: 27115     125.0\n",
      "363267    125.0\n",
      "358673    124.0\n",
      "364750    124.0\n",
      "404768    124.0\n",
      "Name: age, dtype: float64\n",
      "Minimum height': 29311     1.0\n",
      "68995     1.0\n",
      "70012     1.0\n",
      "123238    1.0\n",
      "135247    1.0\n",
      "139870    1.0\n",
      "139914    1.0\n",
      "139950    1.0\n",
      "153348    1.0\n",
      "170930    1.0\n",
      "Name: height, dtype: float64, Maximum height: 377897    20175.0\n",
      "124868     9008.0\n",
      "152034     2441.0\n",
      "27925      2215.0\n",
      "6157       2205.0\n",
      "121387     2113.0\n",
      "256935     2000.0\n",
      "51636      1801.0\n",
      "19939      1783.0\n",
      "377383     1764.0\n",
      "Name: height, dtype: float64\n",
      "Minimum weight': 6157     0.0\n",
      "7384     0.0\n",
      "10847    0.0\n",
      "17122    0.0\n",
      "19320    0.0\n",
      "19665    0.0\n",
      "23203    0.0\n",
      "25896    0.0\n",
      "30426    0.0\n",
      "31051    0.0\n",
      "Name: weight, dtype: float64, Maximum weight: 67446     8388607.0\n",
      "361003       7087.0\n",
      "382676        787.0\n",
      "29607         748.0\n",
      "122814        744.0\n",
      "374476        740.0\n",
      "84035         732.0\n",
      "110804        732.0\n",
      "117237        732.0\n",
      "132676        732.0\n",
      "Name: weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum age': {dataframe['age'].nsmallest(5)}, Maximum age: {dataframe['age'].nlargest(5)}\")\n",
    "print(f\"Minimum height': {dataframe['height'].nsmallest(10)}, Maximum height: {dataframe['height'].nlargest(10)}\")\n",
    "print(f\"Minimum weight': {dataframe['weight'].nsmallest(10)}, Maximum weight: {dataframe['weight'].nlargest(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c523edd6a9c147",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Because even by observing ten extremes for the height and weight the values didn't make sense, in the following average values for the entire population were taken"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 Perform Aggregation\n",
    "\n",
    "The script defines bins and labels for 'age,' 'height,' and 'weight' columns, categorizing the numerical values into bins. This provides a more concise representation of the data distribution:\n",
    "Finally, the modified DataFrame, incorporating binning for 'age,' 'height,' and 'weight,' is printed:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59467bf84ccd64eb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a37f1ccdf7063b7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:35:02.747173Z",
     "start_time": "2024-01-09T12:35:02.706395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id             name       region                          team  \\\n",
      "0      2554.0  Antti JÃ¤rvinen   North East                           NaN   \n",
      "1      3517.0              NaN   North East              CrossFit Decoded   \n",
      "2      4691.0              NaN  Canada West  CrossFit Fire City Singapore   \n",
      "3      5164.0              NaN          NaN                           NaN   \n",
      "\n",
      "                affiliate gender    age   height weight   fran  ...  deadlift  \\\n",
      "0          CrossFit Baruk   Male  19-30  160-169  70-79    NaN  ...     400.0   \n",
      "1          CrossFit Lakás   Male  31-45  190-199  70-79    NaN  ...       NaN   \n",
      "2                     NaN    NaN    NaN      NaN    NaN    NaN  ...       NaN   \n",
      "3  CrossFit Sherwood Park   Male  31-45      NaN  60-69  211.0  ...     375.0   \n",
      "\n",
      "   backsq  pullups                                   eat  \\\n",
      "0   305.0      NaN                                   NaN   \n",
      "1     NaN      NaN                                   NaN   \n",
      "2     NaN      NaN                                   NaN   \n",
      "3   325.0     25.0  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "2                                         NaN        NaN                 NaN  \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN  \n",
      "\n",
      "[4 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "bins_age = [0, 18, 30, 45, 60, 100]\n",
    "labels_age = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "bins_height = [0, 159, 169, 179, 189, 199, 220]\n",
    "labels_height = ['0-159', '160-169', '170-179', '180-189', '190-199', '200+']\n",
    "\n",
    "bins_weight = [0, 49, 59, 69, 79, 89, 99, 130]\n",
    "labels_weight = ['0-49', '50-59', '60-69', '70-79', '80-89', '90-99', '100+']\n",
    "\n",
    "dataframe['age'] = pd.cut(dataframe['age'], bins=bins_age, labels=labels_age)\n",
    "dataframe['height'] = pd.cut(dataframe['height'], bins=bins_height, labels=labels_height)\n",
    "dataframe['weight'] = pd.cut(dataframe['weight'], bins=bins_weight, labels=labels_weight)\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1f6ae44fdd428",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.4 Perturbation\n",
    "Goal:\n",
    "- Select attributes to add noise to (3.4.1)\n",
    "- Implement the functionality to add the noise (3.4.2)\n",
    "    - The original distribution of values should be preserved \n",
    "- Analyse distribution of original data and then with noise added (3.4.3)\n",
    "\n",
    "### 3.4.1 Select attributes\n",
    "- Similar to the aggregation, perturbation as well fits best for numbers as attribute values \n",
    "- Because the stats of the athletes are the main subject of the dataset they should normally be no noise added to them\n",
    "    - Even more, the correctness of the value really matters in order to be able to compare different athletes. Here already a small noise is not good\n",
    "- However, as already mentioned in 3.1 this kind of information loss is accepted in this assignment\n",
    "- Therefore, the stats will be pertubated \n",
    "\n",
    "### 3.4.2 Pertubate Values\n",
    "- Standard deviation was used to determine noise\n",
    "\n",
    "The initial step involves loading the athlete dataset ('athletes.csv') using the pandas library:\n",
    "Continuous variables to be perturbed are explicitly specified:\n",
    "Statistical properties (mean and standard deviation) of non-missing numeric values in the selected columns are analyzed before perturbation:\n",
    "Non-missing numeric values in each specified column are perturbed by adding random noise, determined by a noise factor (in this case, 40). The perturbed data is then rounded and updated in the DataFrame:\n",
    "Statistical properties of non-missing numeric values are re-analyzed after perturbation:\n",
    "The final step involves saving the perturbed data back to the original CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258df58c6fdb35f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.078982Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from the CSV file\n",
    "#dataframe = pd.read_csv('athletes.csv')\n",
    "\n",
    "columns_to_perturb = ['pullups', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq']\n",
    "\n",
    "# Analyze the non-missing numeric values before perturbation\n",
    "stats_before_perturbation = {}\n",
    "for column in columns_to_perturb:\n",
    "    data = dataframe[column].values\n",
    "    non_missing_values = [value for value in data if not pd.isna(value)]\n",
    "    mean_before = np.mean(non_missing_values)\n",
    "    std_dev_before = np.std(non_missing_values)\n",
    "    stats_before_perturbation[column] = {'mean_before': mean_before, 'std_dev_before': std_dev_before}\n",
    "\n",
    "print(\"Before Perturbation:\")\n",
    "for column, stats in stats_before_perturbation.items():\n",
    "    print(f\"Mean {column} (Before): {stats['mean_before']}\")\n",
    "    print(f\"Std Dev {column} (Before): {stats['std_dev_before']}\")\n",
    "\n",
    "# Choose a noise factor\n",
    "noise_factor = 40  # Adjust this based on your privacy requirements\n",
    "\n",
    "# Perturb the non-missing numeric values for each specified column\n",
    "for column in columns_to_perturb:\n",
    "    perturbed_data = []\n",
    "    for value in dataframe[column]:\n",
    "        if not pd.isna(value):\n",
    "            perturbed_value = value + np.random.normal(0, noise_factor)\n",
    "            perturbed_value = int(abs(round(perturbed_value)))\n",
    "            perturbed_data.append(perturbed_value)\n",
    "        else:\n",
    "            perturbed_data.append(np.nan)\n",
    "    dataframe[column] = perturbed_data  # Update the column in the DataFrame with perturbed data\n",
    "\n",
    "\n",
    "# Save the perturbed data back to a CSV file without scientific notation\n",
    "dataframe.to_csv('athletes_anonyzmized.csv', index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 Analyze after Pertubation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9608137f7b0e90b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze the non-missing numeric values after perturbation\n",
    "stats_after_perturbation = {}\n",
    "for column in columns_to_perturb:\n",
    "    non_missing_values_after = [value for value in dataframe[column] if not pd.isna(value)]\n",
    "    mean_after = np.mean(non_missing_values_after)\n",
    "    std_dev_after = np.std(non_missing_values_after)\n",
    "    stats_after_perturbation[column] = {'mean_after': mean_after, 'std_dev_after': std_dev_after}\n",
    "\n",
    "print(\"\\nAfter Perturbation:\")\n",
    "for column, stats in stats_after_perturbation.items():\n",
    "    print(f\"Mean {column} (After): {stats['mean_after']}\")\n",
    "    print(f\"Std Dev {column} (After): {stats['std_dev_after']}\")\n",
    "    \n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20,10))\n",
    "ax1.hist(\n",
    "    'helen', \n",
    "    data=pd.read_csv('athletes.csv'),\n",
    "    bins = np.arange(start=100, stop=2_000, step=200),\n",
    ")\n",
    "ax1.title.set_text('Original Distribution')\n",
    "ax2.hist(\n",
    "    'helen', \n",
    "    data=dataframe, \n",
    "    bins = np.arange(start=100, stop=2_000, step=200),\n",
    ")\n",
    "ax2.title.set_text('Distribution with noise added')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.079979200Z"
    }
   },
   "id": "d8f6b2643b725d7e"
  },
  {
   "cell_type": "markdown",
   "id": "e3084722e47e63b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.5 Data Analysis\n",
    "Goal:\n",
    "- Determine data loss using function (3.5.1)\n",
    "- Discuss pros and cons (3.5.2)\n",
    "- attributes included in the information loss function: ['pullups', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq'] \n",
    "- maybe aggregated values: age, height, weight, but they are not continious variables, so they wont be respected in the following informatiin loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The initial step involves reading two CSV files, 'athletes.csv' and 'athletes_og.csv', using the pandas library, in which the athletes_og.csv is the original file and the athletes.csv is the file containing the perturbed continuous values.\n",
    "Aggregated, randomized and pseudonimized are not continuous and are therefore not considered in the Data Analysis.\n",
    "The Continuous variables to be analysed are explicitly specified.\n",
    "A function named 'calculate_information_loss' is defined to calculate the information loss between two arrays using the mean squared differences and a scaling factor.\n",
    "Information loss is calculated for each column, aggregated, and then the total information loss is printed:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a141aa81ac401563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV files\n",
    "\n",
    "original_data = pd.read_csv(\"athletes.csv\")\n",
    "anonymized_data = pd.read_csv(\"athletes_anonyzmized.csv\")\n",
    "\n",
    "# Columns to calculate information loss\n",
    "columns_to_compare = ['pullups', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq']\n",
    "\n",
    "# Calculate Information Loss\n",
    "def calculate_information_loss(x, y, S):\n",
    "    return (x - y) / np.sqrt(2 * S)\n",
    "\n",
    "total_information_loss = 0\n",
    "n = len(original_data)  # Assuming both datasets have the same number of rows\n",
    "p = len(columns_to_compare)  # Number of columns to compare\n",
    "\n",
    "for column in columns_to_compare:\n",
    "    x = original_data[column]\n",
    "    y = anonymized_data[column]\n",
    "    S = np.mean((x - y) ** 2)  # Variance of the differences in the column\n",
    "\n",
    "    information_loss = np.sum(calculate_information_loss(x, y, S)) / (p * n)\n",
    "    total_information_loss += information_loss\n",
    "\n",
    "print(\"Total Information Loss:\", total_information_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:28.088268300Z",
     "start_time": "2024-01-09T08:42:28.080982400Z"
    }
   },
   "id": "66c50d3e82868de8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.081982600Z"
    }
   },
   "id": "a9b38097fd44d7be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
