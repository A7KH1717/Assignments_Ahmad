{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #1: Pseudonymisation Techniques and Considerations\n",
    "- Dataset: Crossfit [Daset](https://data.world/bgadoci/crossfit-data) (In this assignment only the athletes file was used) \n",
    "- Credits: Dataset was put together by Sam Swift\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd0b2ac563f828dd"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be93d85a454a3b2d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:10.814981200Z",
     "start_time": "2024-01-09T08:42:09.524607900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id           name               region          team  \\\n",
      "0      2554.0      Pj Ablang           South West   Double Edge   \n",
      "1      3517.0  Derek Abdella                  NaN           NaN   \n",
      "2      4691.0            NaN                  NaN           NaN   \n",
      "3      5164.0    Abo Brandon  Southern California  LAX CrossFit   \n",
      "\n",
      "              affiliate gender   age  height  weight   fran  ...  deadlift  \\\n",
      "0  Double Edge CrossFit   Male  24.0   166.0    70.0    NaN  ...     400.0   \n",
      "1                   NaN   Male  42.0   190.0    70.0    NaN  ...       NaN   \n",
      "2                   NaN    NaN   NaN     NaN     NaN    NaN  ...       NaN   \n",
      "3          LAX CrossFit   Male  40.0     NaN    67.0  211.0  ...     375.0   \n",
      "\n",
      "   backsq  pullups                                   eat  \\\n",
      "0   305.0      NaN                                   NaN   \n",
      "1     NaN      NaN                                   NaN   \n",
      "2     NaN      NaN                                   NaN   \n",
      "3   325.0     25.0  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN  \n",
      "2                                         NaN        NaN                 NaN  \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN  \n",
      "\n",
      "[4 rows x 28 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julius\\AppData\\Local\\Temp\\ipykernel_14800\\3265770945.py:4: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(\"athletes.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read csv as dataframe\n",
    "dataframe = pd.read_csv(\"athletes.csv\")\n",
    "\n",
    "#dataframe[\"test\"] = dataframe[\"height\"]\n",
    "#dataframe[\"height\"] = dataframe[\"weight\"]\n",
    "#dataframe[\"weight\"] = dataframe[\"test\"]\n",
    "\n",
    "#dataframe = dataframe.drop(columns=['test'])\n",
    "\n",
    "#dataframe.to_csv('athletes_new.csv', index=False)\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "## 3.1 Pseudonymisation\n",
    "Goals:\n",
    "-  Determine which attributes qualify as explicit personally identifiable information (3.1.1)\n",
    "    - Why? \n",
    "    - What method was used to identify these? \n",
    "- Generate pseudonymous values for the identified attributes (3.1.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b64615a1710ba2a"
  },
  {
   "cell_type": "markdown",
   "id": "a09a389f01e7c40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1.1 Identifying Attributes containing Personally Identifiable Information (PII)\n",
    "- In order to identify th attributes first a better understanding of the structure of the dataset needs to be obtained\n",
    "    - What columns does the dataset contain and in what format are the attribute values?\n",
    "        - Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 'athlete_id', Example Data: 2554.0\n",
      "Column: 'name', Example Data: Pj Ablang\n",
      "Column: 'region', Example Data: South West\n",
      "Column: 'team', Example Data: Double Edge\n",
      "Column: 'affiliate', Example Data: Double Edge CrossFit\n",
      "Column: 'gender', Example Data: Male\n",
      "Column: 'age', Example Data: 24.0\n",
      "Column: 'height', Example Data: 166.0\n",
      "Column: 'weight', Example Data: 70.0\n",
      "Column: 'fran', Example Data: 211.0\n",
      "Column: 'helen', Example Data: 645.0\n",
      "Column: 'grace', Example Data: 300.0\n",
      "Column: 'filthy50', Example Data: 1053.0\n",
      "Column: 'fgonebad', Example Data: 0.0\n",
      "Column: 'run400', Example Data: 61.0\n",
      "Column: 'run5k', Example Data: 1081.0\n",
      "Column: 'candj', Example Data: 220.0\n",
      "Column: 'snatch', Example Data: 200.0\n",
      "Column: 'deadlift', Example Data: 400.0\n",
      "Column: 'backsq', Example Data: 305.0\n",
      "Column: 'pullups', Example Data: 25.0\n",
      "Column: 'eat', Example Data: I eat 1-3 full cheat meals per week|\n",
      "Column: 'train', Example Data: I workout mostly at a CrossFit Affiliate|I have a coach who determines my programming|I record my workouts|\n",
      "Column: 'background', Example Data: I played youth or high school level sports|I regularly play recreational sports|\n",
      "Column: 'experience', Example Data: I began CrossFit with a coach (e.g. at an affiliate)|I have attended one or more specialty courses|I have had a life changing experience due to CrossFit|\n",
      "Column: 'schedule', Example Data: I do multiple workouts in a day 2x a week|\n",
      "Column: 'howlong', Example Data: 4+ years|\n",
      "Column: 'retrieved_datetime', Example Data: 2015-03-01 22:49:18\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_not_empty_value(df_column):\n",
    "    return df_column.dropna().iloc[0] if not df_column.dropna().empty else None\n",
    "\n",
    "# Iterate each column \n",
    "for column in dataframe.columns:\n",
    "    first_value = get_first_not_not_empty_value(dataframe[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:11.166832900Z",
     "start_time": "2024-01-09T08:42:10.817489600Z"
    }
   },
   "id": "3669191d576f3aae"
  },
  {
   "cell_type": "markdown",
   "id": "6ed2670d051f539a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- By inspecting the different columns and the data format, several attributes which have the potential to contain explicit personally identifiable information can be identified\n",
    "    - `athelete_id`\n",
    "        -  This really depends on the usage of this id! Considerations to take into account are: \n",
    "            - Is the `athlete_id` only used as an internal id of this dataset or does it maybe even refer to an official id?\n",
    "            - Are there other datasets available which may have a similar source to this dataset? Thus, these other datasets may use the same `athlete_id`\n",
    "    - `name`\n",
    "        - The name allows to identify an individual\n",
    "    - `team`\n",
    "        - Depending on the size of the team, this could allow to identify a specific athlete\n",
    "    - `affiliate` \n",
    "        - Depending on the affiliate and the amount of contracted athletes, this could allow to identify an individual\n",
    "    - All stats of the athletes\n",
    "        - If an athlete has really remarkable stats (maybe even a world record in a category), this could allow to identify the individual\n",
    "    - `train` \n",
    "        - If an athlete has a special and famous training routine, this could allow to identify him\n",
    "    - `background`\n",
    "        - If an athlete has a famous background or mentions names, this could allow to identify him\n",
    "    - `experience`\n",
    "        - If an athlete mentions concrete information about his experience (e.g. name of current coach), this could allow to identify him\n",
    "\n",
    "-> As can be seen, all columns could potentially contain outliers which could be then used to identify an individual. As agreed on, for each of the columns (only those holding numbers or names) potentially leaking PII one anonymization technique was chosen which keeps the data loss as minimal as possible without leaking obvious PII. It is important to note, that there could still be columns containing PII. However, this chance for outliers was accepted in this assignment. Columns containing descriptions (e.g. train, background and experience) where to anonymized and thus the chance for outliers accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e7f8a58b909df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1.2 Pseudonymizing\n",
    "- For pseudonymisation especially the name is suitable, because it can be easily replaced with a fake name, without being as obvious or destroying the purpose of the dataset\n",
    "- With the help of the `anonymizedf` library new fake names can be created for the `name` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1f75b29dbfdecd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:21.880908400Z",
     "start_time": "2024-01-09T08:42:11.163837900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   athlete_id               region          team             affiliate gender  \\\n",
      "0      2554.0           South West   Double Edge  Double Edge CrossFit   Male   \n",
      "1      3517.0                  NaN           NaN                   NaN   Male   \n",
      "2      4691.0                  NaN           NaN                   NaN    NaN   \n",
      "3      5164.0  Southern California  LAX CrossFit          LAX CrossFit   Male   \n",
      "\n",
      "    age  height  weight   fran  helen  ...  backsq  pullups  \\\n",
      "0  24.0   166.0    70.0    NaN    NaN  ...   305.0      NaN   \n",
      "1  42.0   190.0    70.0    NaN    NaN  ...     NaN      NaN   \n",
      "2   NaN     NaN     NaN    NaN    NaN  ...     NaN      NaN   \n",
      "3  40.0     NaN    67.0  211.0  645.0  ...   325.0     25.0   \n",
      "\n",
      "                                    eat  \\\n",
      "0                                   NaN   \n",
      "1                                   NaN   \n",
      "2                                   NaN   \n",
      "3  I eat 1-3 full cheat meals per week|   \n",
      "\n",
      "                                               train  \\\n",
      "0  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "1  I have a coach who determines my programming|I...   \n",
      "2                                                NaN   \n",
      "3  I workout mostly at a CrossFit Affiliate|I hav...   \n",
      "\n",
      "                                          background  \\\n",
      "0  I played youth or high school level sports|I r...   \n",
      "1        I played youth or high school level sports|   \n",
      "2                                                NaN   \n",
      "3        I played youth or high school level sports|   \n",
      "\n",
      "                                          experience  \\\n",
      "0  I began CrossFit with a coach (e.g. at an affi...   \n",
      "1  I began CrossFit with a coach (e.g. at an affi...   \n",
      "2                                                NaN   \n",
      "3  I began CrossFit by trying it alone (without a...   \n",
      "\n",
      "                                     schedule    howlong  retrieved_datetime  \\\n",
      "0  I do multiple workouts in a day 2x a week|  4+ years|                 NaN   \n",
      "1  I do multiple workouts in a day 2x a week|  4+ years|                 NaN   \n",
      "2                                         NaN        NaN                 NaN   \n",
      "3          I usually only do 1 workout a day|  4+ years|                 NaN   \n",
      "\n",
      "              name  \n",
      "0       Ross Smith  \n",
      "1     Arthur Poole  \n",
      "2  Jeremy Thompson  \n",
      "3  Mr Ben Phillips  \n",
      "\n",
      "[4 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "from anonymizedf import anonymizedf\n",
    "\n",
    "# Prepare the data to be anonymized\n",
    "an = anonymizedf.anonymize(dataframe)\n",
    "\n",
    "# Add new column with fake name\n",
    "an.fake_names(\"name\")\n",
    "\n",
    "# Drop old original name column\n",
    "dataframe = dataframe.drop(columns=['name'])\n",
    "\n",
    "# Rename Fake_name column in name\n",
    "dataframe = dataframe.rename(columns={'Fake_name': 'name'})\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d638b8091b0fee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 Randomisation\n",
    "Goal:\n",
    "- Use randomisation technique to generate random strings (no meaning) (3.2.1)\n",
    "- Use randomisation technique to generate random but meaningful replacements and create lookup table (3.2.2)\n",
    "\n",
    "### 3.2.1 Random Strings\n",
    "1. Identify columns to replace with random strings\n",
    "    - Here columns containing words can be used \n",
    "        - `name`\n",
    "        - `region`\n",
    "        - `team`\n",
    "        - `affiliate` \n",
    "2. Generate a function which is responsible for generating random values for replacement\n",
    "3. Overwrite each value in respective attributes by using the generated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea49738582c6a6e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:28.070850400Z",
     "start_time": "2024-01-09T08:42:21.874877300Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregion\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: generate_random_string(\u001B[38;5;241m15\u001B[39m))\n\u001B[0;32m     12\u001B[0m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteam\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteam\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: generate_random_string(\u001B[38;5;241m20\u001B[39m))\n\u001B[1;32m---> 13\u001B[0m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maffiliate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdataframe\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maffiliate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerate_random_string\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataframe\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m4\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4760\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4625\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4626\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4627\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4632\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4633\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4634\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4635\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4636\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4751\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4752\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4753\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4754\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4755\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4756\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4757\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4758\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4760\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1207\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1206\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1287\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1281\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1282\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1283\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1285\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1286\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1287\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1288\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1289\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1292\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1293\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1812\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1814\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1815\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1816\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1817\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1818\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2920\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[4], line 13\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     11\u001B[0m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregion\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: generate_random_string(\u001B[38;5;241m15\u001B[39m))\n\u001B[0;32m     12\u001B[0m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteam\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteam\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: generate_random_string(\u001B[38;5;241m20\u001B[39m))\n\u001B[1;32m---> 13\u001B[0m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maffiliate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maffiliate\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mgenerate_random_string\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataframe\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m4\u001B[39m])\n",
      "Cell \u001B[1;32mIn[4], line 7\u001B[0m, in \u001B[0;36mgenerate_random_string\u001B[1;34m(length)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_random_string\u001B[39m(length):\n\u001B[0;32m      6\u001B[0m     letters \u001B[38;5;241m=\u001B[39m string\u001B[38;5;241m.\u001B[39mascii_letters\n\u001B[1;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(random\u001B[38;5;241m.\u001B[39mchoice(letters) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(length))\n",
      "Cell \u001B[1;32mIn[4], line 7\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_random_string\u001B[39m(length):\n\u001B[0;32m      6\u001B[0m     letters \u001B[38;5;241m=\u001B[39m string\u001B[38;5;241m.\u001B[39mascii_letters\n\u001B[1;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(random\u001B[38;5;241m.\u001B[39mchoice(letters) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(length))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Define function to create random string\n",
    "def generate_random_string(length):\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "# Apply randomization on specified columns\n",
    "dataframe['name'] = dataframe['name'].apply(lambda x: generate_random_string(10))\n",
    "dataframe['region'] = dataframe['region'].apply(lambda x: generate_random_string(15))\n",
    "dataframe['team'] = dataframe['team'].apply(lambda x: generate_random_string(20))\n",
    "dataframe['affiliate'] = dataframe['affiliate'].apply(lambda x: generate_random_string(25))\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72fe81893e96ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2.2 Randomization with meaningful strings\n",
    "1. Identify columns suitable\n",
    "    - name, region, team, affiliate\n",
    "        -  For randomizing the name, the first letter will be kept equal to the original in order to keep similarities\n",
    "2. Write functionality to randomize the values\n",
    "- Interchange Values in Columns Name, Region, Team, Affiliate with meaningful strings to randomize and print out the lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To see the changes properly we have to reload the initial data\n",
    "dataframe = pd.read_csv(\"athletes.csv\")\n",
    "\n",
    "# Copy the original values in mapping dataframe\n",
    "dataframe_mapping = dataframe[['name', 'region', 'team', 'affiliate']].copy()\n",
    "\n",
    "# Interchange values for region, team and affiliate\n",
    "def interchange_values(column_name):\n",
    "    dataframe[column_name] = dataframe[column_name].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Randomize name, region, team and affiliate\n",
    "interchange_values('name')\n",
    "interchange_values('region')\n",
    "interchange_values('team')\n",
    "interchange_values('affiliate')\n",
    "\n",
    "# Copy the modified values in mapping dataframe\n",
    "dataframe_mapping[['randomized_name', 'randomized_region', 'randomized_team', 'randomized_affiliate']] = dataframe[['name', 'region', 'team', 'affiliate']].copy()\n",
    "\n",
    "# Save mapping to csv\n",
    "dataframe_mapping.to_csv('athletes_mapping.csv', index=False)\n",
    "\n",
    "print(dataframe.iloc[:4])\n",
    "print(dataframe_mapping.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:28.073338900Z",
     "start_time": "2024-01-09T08:42:28.071924400Z"
    }
   },
   "id": "fe9526ff779442bc"
  },
  {
   "cell_type": "markdown",
   "id": "c59dc5a07acdb83d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 Aggregation\n",
    "Goal:\n",
    "- Determine attributes which qualify for aggregation (3.3.1)\n",
    "- Write a function to perform that aggregation process (3.3.2)\n",
    "\n",
    "### 3.3.1 Determine Attributes for Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The script iterates through each column in the DataFrame and prints the column name along with the first non-empty value in that column:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42c548ad3a5e556c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc0b43c8b43a28",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.073338900Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_first_not_not_empty_value(df_column):\n",
    "    return df_column.dropna().iloc[0] if not df_column.dropna().empty else None\n",
    "\n",
    "# Print each column with its first not empty value\n",
    "for column in dataframe.columns:\n",
    "    first_value = get_first_not_not_empty_value(dataframe[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c31ee64889069",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Numerical attributes can be easily aggregated\n",
    "- Especially those related to information about the individual are suitable for aggregation\n",
    "    - `age`\n",
    "    - `height` \n",
    "    - `weight`\n",
    "- To be able to identify different levels of values, the extremes have to be identified "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The script prints the minimum and maximum values for 'age,' 'height,' and 'weight' columns:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5826be06ba973a42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc635f8ab91128",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.074499200Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Minimum age': {dataframe['age'].nsmallest(5)}, Maximum age: {dataframe['age'].nlargest(5)}\")\n",
    "print(f\"Minimum height': {dataframe['height'].nsmallest(10)}, Maximum height: {dataframe['height'].nlargest(10)}\")\n",
    "print(f\"Minimum weight': {dataframe['weight'].nsmallest(10)}, Maximum weight: {dataframe['weight'].nlargest(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c523edd6a9c147",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Because even by observing ten extremes for the height and weight the values didn't make sense, in the following average values for the entire population were taken"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 Perform Aggregation\n",
    "\n",
    "The script defines bins and labels for 'age,' 'height,' and 'weight' columns, categorizing the numerical values into bins. This provides a more concise representation of the data distribution:\n",
    "Finally, the modified DataFrame, incorporating binning for 'age,' 'height,' and 'weight,' is printed:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59467bf84ccd64eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37f1ccdf7063b7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.075700100Z"
    }
   },
   "outputs": [],
   "source": [
    "bins_age = [0, 18, 30, 45, 60, 100]\n",
    "labels_age = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "bins_height = [0, 159, 169, 179, 189, 199, 220]\n",
    "labels_height = ['0-159', '160-169', '170-179', '180-189', '190-199', '200+']\n",
    "\n",
    "bins_weight = [0, 49, 59, 69, 79, 89, 99, 130]\n",
    "labels_weight = ['0-49', '50-59', '60-69', '70-79', '80-89', '90-99', '100+']\n",
    "\n",
    "dataframe['age'] = pd.cut(dataframe['age'], bins=bins_age, labels=labels_age)\n",
    "dataframe['height'] = pd.cut(dataframe['height'], bins=bins_height, labels=labels_height)\n",
    "dataframe['weight'] = pd.cut(dataframe['weight'], bins=bins_weight, labels=labels_weight)\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1f6ae44fdd428",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.4 Perturbation\n",
    "Goal:\n",
    "- Select attributes to add noise to (3.4.1)\n",
    "- Implement the functionality to add the noise (3.4.2)\n",
    "    - The original distribution of values should be preserved \n",
    "- Analyse distribution of original data and then with noise added (3.4.3)\n",
    "\n",
    "### 3.4.1 Select attributes\n",
    "- Similar to the aggregation, perturbation as well fits best for numbers as attribute values \n",
    "- Because the stats of the athletes are the main subject of the dataset they should normally be no noise added to them\n",
    "    - Even more, the correctness of the value really matters in order to be able to compare different athletes. Here already a small noise is not good\n",
    "- However, as already mentioned in 3.1 this kind of information loss is accepted in this assignment\n",
    "- Therefore, the stats will be pertubated \n",
    "\n",
    "### 3.4.2 Pertubate Values\n",
    "- Standard deviation was used to determine noise\n",
    "\n",
    "The initial step involves loading the athlete dataset ('athletes.csv') using the pandas library:\n",
    "Continuous variables to be perturbed are explicitly specified:\n",
    "Statistical properties (mean and standard deviation) of non-missing numeric values in the selected columns are analyzed before perturbation:\n",
    "Non-missing numeric values in each specified column are perturbed by adding random noise, determined by a noise factor (in this case, 40). The perturbed data is then rounded and updated in the DataFrame:\n",
    "Statistical properties of non-missing numeric values are re-analyzed after perturbation:\n",
    "The final step involves saving the perturbed data back to the original CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258df58c6fdb35f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.078982Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from the CSV file\n",
    "#dataframe = pd.read_csv('athletes.csv')\n",
    "\n",
    "columns_to_perturb = ['pullups', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq']\n",
    "\n",
    "# Analyze the non-missing numeric values before perturbation\n",
    "stats_before_perturbation = {}\n",
    "for column in columns_to_perturb:\n",
    "    data = dataframe[column].values\n",
    "    non_missing_values = [value for value in data if not pd.isna(value)]\n",
    "    mean_before = np.mean(non_missing_values)\n",
    "    std_dev_before = np.std(non_missing_values)\n",
    "    stats_before_perturbation[column] = {'mean_before': mean_before, 'std_dev_before': std_dev_before}\n",
    "\n",
    "print(\"Before Perturbation:\")\n",
    "for column, stats in stats_before_perturbation.items():\n",
    "    print(f\"Mean {column} (Before): {stats['mean_before']}\")\n",
    "    print(f\"Std Dev {column} (Before): {stats['std_dev_before']}\")\n",
    "\n",
    "# Choose a noise factor\n",
    "noise_factor = 40  # Adjust this based on your privacy requirements\n",
    "\n",
    "# Perturb the non-missing numeric values for each specified column\n",
    "for column in columns_to_perturb:\n",
    "    perturbed_data = []\n",
    "    for value in dataframe[column]:\n",
    "        if not pd.isna(value):\n",
    "            perturbed_value = value + np.random.normal(0, noise_factor)\n",
    "            perturbed_value = int(abs(round(perturbed_value)))\n",
    "            perturbed_data.append(perturbed_value)\n",
    "        else:\n",
    "            perturbed_data.append(np.nan)\n",
    "    dataframe[column] = perturbed_data  # Update the column in the DataFrame with perturbed data\n",
    "\n",
    "\n",
    "# Save the perturbed data back to a CSV file without scientific notation\n",
    "dataframe.to_csv('athletes_anonyzmized.csv', index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 Analyze after Pertubation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9608137f7b0e90b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze the non-missing numeric values after perturbation\n",
    "stats_after_perturbation = {}\n",
    "for column in columns_to_perturb:\n",
    "    non_missing_values_after = [value for value in dataframe[column] if not pd.isna(value)]\n",
    "    mean_after = np.mean(non_missing_values_after)\n",
    "    std_dev_after = np.std(non_missing_values_after)\n",
    "    stats_after_perturbation[column] = {'mean_after': mean_after, 'std_dev_after': std_dev_after}\n",
    "\n",
    "print(\"\\nAfter Perturbation:\")\n",
    "for column, stats in stats_after_perturbation.items():\n",
    "    print(f\"Mean {column} (After): {stats['mean_after']}\")\n",
    "    print(f\"Std Dev {column} (After): {stats['std_dev_after']}\")\n",
    "    \n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20,10))\n",
    "ax1.hist(\n",
    "    'helen', \n",
    "    data=pd.read_csv('athletes.csv'),\n",
    "    bins = np.arange(start=100, stop=2_000, step=200),\n",
    ")\n",
    "ax1.title.set_text('Original Distribution')\n",
    "ax2.hist(\n",
    "    'helen', \n",
    "    data=dataframe, \n",
    "    bins = np.arange(start=100, stop=2_000, step=200),\n",
    ")\n",
    "ax2.title.set_text('Distribution with noise added')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.079979200Z"
    }
   },
   "id": "d8f6b2643b725d7e"
  },
  {
   "cell_type": "markdown",
   "id": "e3084722e47e63b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.5 Data Analysis\n",
    "Goal:\n",
    "- Determine data loss using function (3.5.1)\n",
    "- Discuss pros and cons (3.5.2)\n",
    "- attributes included in the information loss function: ['pullups', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq'] \n",
    "- maybe aggregated values: age, height, weight, but they are not continious variables, so they wont be respected in the following informatiin loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The initial step involves reading two CSV files, 'athletes.csv' and 'athletes_og.csv', using the pandas library, in which the athletes_og.csv is the original file and the athletes.csv is the file containing the perturbed continuous values.\n",
    "Aggregated, randomized and pseudonimized are not continuous and are therefore not considered in the Data Analysis.\n",
    "The Continuous variables to be analysed are explicitly specified.\n",
    "A function named 'calculate_information_loss' is defined to calculate the information loss between two arrays using the mean squared differences and a scaling factor.\n",
    "Information loss is calculated for each column, aggregated, and then the total information loss is printed:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a141aa81ac401563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV files\n",
    "\n",
    "original_data = pd.read_csv(\"athletes.csv\")\n",
    "anonymized_data = pd.read_csv(\"athletes_anonyzmized.csv\")\n",
    "\n",
    "# Columns to calculate information loss\n",
    "columns_to_compare = ['pullups', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq']\n",
    "\n",
    "# Calculate Information Loss\n",
    "def calculate_information_loss(x, y, S):\n",
    "    return (x - y) / np.sqrt(2 * S)\n",
    "\n",
    "total_information_loss = 0\n",
    "n = len(original_data)  # Assuming both datasets have the same number of rows\n",
    "p = len(columns_to_compare)  # Number of columns to compare\n",
    "\n",
    "for column in columns_to_compare:\n",
    "    x = original_data[column]\n",
    "    y = anonymized_data[column]\n",
    "    S = np.mean((x - y) ** 2)  # Variance of the differences in the column\n",
    "\n",
    "    information_loss = np.sum(calculate_information_loss(x, y, S)) / (p * n)\n",
    "    total_information_loss += information_loss\n",
    "\n",
    "print(\"Total Information Loss:\", total_information_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T08:42:28.088268300Z",
     "start_time": "2024-01-09T08:42:28.080982400Z"
    }
   },
   "id": "66c50d3e82868de8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T08:42:28.081982600Z"
    }
   },
   "id": "a9b38097fd44d7be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
