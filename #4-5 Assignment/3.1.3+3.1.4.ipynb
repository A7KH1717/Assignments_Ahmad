{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Using the techniques you applied in Assignment #1, apply a masking or transformation mechanism to modify the detected PII elements and substitute with suitable replacements.\n",
    "In the following section, we will apply techniques similar to those used in Assignment #1 to mask or transform Personally Identifiable Information (PII) detected in a dataset. The goal is to substitute these sensitive elements with suitable replacements while maintaining the overall structure and coherence of the data. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf64464ca6c1a3f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by anonymizing the eight categories with help from the Faker library:\n",
    "- PERSON: Replaces names of people with fake names generated by Faker.\n",
    "- GPE (Geopolitical Entities): Substitutes names of countries, cities, states, etc., with random city names using Faker.\n",
    "- DATE: Transforms dates into random dates. Specific dates like 'today', 'tomorrow', or 'next week' are replaced with dates that correspond to these descriptions.\n",
    "- ORG (Organizations): Changes names of organizations, companies, agencies, etc., to random company names generated by Faker.\n",
    "- NORP (Nationalities, Religious or Political Groups): Replaces nationalities, religions, and political group names with random country names, implying a change in nationality.\n",
    "- CARDINAL (Numerals): Alters numerical values to be close to the original number but not exact, within a ±10% range or ±3, whichever is greater.\n",
    "- ORDINAL (Ordinal Numbers): Generates random ordinal numbers (like 1st, 2nd, 3rd, etc.) to replace existing ordinal numbers.\n",
    "- TIME: Changes time mentions to random times. Specific times of the day like 'morning' or 'evening' are replaced with times corresponding to those periods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8197488583d6da4"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:07:40.857739Z",
     "start_time": "2024-01-28T14:07:40.774309Z"
    }
   },
   "id": "521a2db1d5ace81c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:07:42.058167Z",
     "start_time": "2024-01-28T14:07:40.860068Z"
    }
   },
   "id": "b05743fe8a77ffb9"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "import re\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def close_number(original_number):\n",
    "    try:\n",
    "        num = int(original_number)\n",
    "        # Generate a number within ±10% of the original number or ±3, whichever is greater\n",
    "        percentage_variation = int(num * 0.1)\n",
    "        min_variation = 3  # Minimum variation\n",
    "        variation = max(min_variation, percentage_variation)\n",
    "        return str(random.randint(max(0, num - variation), num + variation))\n",
    "    except ValueError:\n",
    "        # Return the original number if it's not an integer\n",
    "        return original_number\n",
    "    \n",
    "def fake_ordinal():\n",
    "    number = fake.random_int(min=1, max=100)\n",
    "    suffix = [\"th\", \"st\", \"nd\", \"rd\"] + [\"th\"] * 6\n",
    "    return str(number) + suffix[number % 10 if number % 100 not in [11, 12, 13] else 0]\n",
    "    \n",
    "def replace_date(entity_text):\n",
    "    today = datetime.today()\n",
    "    if entity_text.lower() in ['today']:\n",
    "        new_date = fake.date_between(start_date=today, end_date=today)\n",
    "    elif entity_text.lower() in ['tomorrow']:\n",
    "        new_date = fake.date_between(start_date=today + timedelta(days=1), end_date=today + timedelta(days=1))\n",
    "    elif entity_text.lower() in ['next week']:\n",
    "        new_date = fake.date_between(start_date=today + timedelta(days=7), end_date=today + timedelta(days=14))\n",
    "    # Add more conditions for other specific date phrases\n",
    "    else:\n",
    "        # For general date entities, return a random future date\n",
    "        new_date = fake.future_date()\n",
    "    \n",
    "    return new_date.strftime(\"%Y-%m-%d\")  # Convert the date to a string\n",
    "\n",
    "\n",
    "def replace_time(entity_text):\n",
    "    # Define time ranges\n",
    "    morning_times = [f\"{hour:02d}:{minute:02d} AM\" for hour in range(6, 12) for minute in range(0, 60)]\n",
    "    evening_times = [f\"{hour:02d}:{minute:02d} PM\" for hour in range(6, 12) for minute in range(0, 60)]\n",
    "\n",
    "    if entity_text.lower() in ['morning']:\n",
    "        return random.choice(morning_times)\n",
    "    elif entity_text.lower() in ['evening', 'tonight']:\n",
    "        return random.choice(evening_times)\n",
    "    else:\n",
    "        # For general time entities, return any random time\n",
    "        return fake.time()\n",
    "\n",
    "def replace_pii_with_fake(text):\n",
    "    # Replace Twitter @username with @ followed by a fake first name\n",
    "    text = re.sub(r'@(\\w+)', lambda x: '@' + fake.first_name(), text)\n",
    "\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Iterate over the identified entities\n",
    "    for ent in doc.ents:\n",
    "        # Replace with fake data based on the entity type\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = re.sub(re.escape(ent.text), fake.name(), text)\n",
    "        elif ent.label_ == 'GPE':\n",
    "            text = re.sub(re.escape(ent.text), fake.city(), text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            text = re.sub(re.escape(ent.text), replace_date(ent.text), text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            text = re.sub(re.escape(ent.text), fake.company(), text)\n",
    "        elif ent.label_ == 'NORP':\n",
    "            text = re.sub(re.escape(ent.text), fake.country(), text)\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            text = re.sub(re.escape(ent.text), lambda x: close_number(ent.text), text)\n",
    "        elif ent.label_ == 'ORDINAL':\n",
    "            text = re.sub(re.escape(ent.text), fake_ordinal(), text)\n",
    "        elif ent.label_ == 'TIME':\n",
    "            text = re.sub(re.escape(ent.text), replace_time(ent.text), text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['content'] = df['content'].apply(replace_pii_with_fake)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:10:43.655531Z",
     "start_time": "2024-01-28T14:07:42.067048Z"
    }
   },
   "id": "3e67063453962912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "That being done, we take a look at our anonymized dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a082ca24a87464fd"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  @Vincent i know  i was listenin to bad habit e...   \n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696     sadness            Funeral ceremony...gloomy 2024-01-29...   \n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
      "4  1956968416     neutral  @Madeline We want to trade with someone who ha...   \n",
      "5  1956968477       worry  Re-pinging @Julie: why didn't you go to prom? ...   \n",
      "6  1956968487     sadness  I should be sleep, but im not! thinking about ...   \n",
      "7  1956968636       worry               Hmmm. http://www.djhero.com/ is down   \n",
      "\n",
      "                           PII  \n",
      "0  [('@tiffanylue', 'PERSON')]  \n",
      "1                           []  \n",
      "2         [('friday', 'DATE')]  \n",
      "3                           []  \n",
      "4         [('Houston', 'GPE')]  \n",
      "5           [('BC', 'PERSON')]  \n",
      "6          [('2', 'CARDINAL')]  \n",
      "7                           []  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "print(df.iloc[:8])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:10:43.721074Z",
     "start_time": "2024-01-28T14:10:43.655259Z"
    }
   },
   "id": "3517b896a417891d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "44e32a3afb85bedc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Analyse the text to determine if any information can be obtained after the transformation process. What conclusions can you draw from this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d24d03b52d742b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idea is to check the semantics of text from the original dataset, the anonymized dataset and see if they are similar. Then, check if the PII from the original dataset are still present in the anonymized dataset\n",
    "\n",
    "Cosine Similarity: Cosine similarity measures the cosine of the angle between two non-zero vectors in a multidimensional space. Its value ranges from -1 to 1, where:\n",
    "\n",
    "1 means the vectors are identical.\n",
    "0 indicates orthogonality (no similarity).\n",
    "-1 implies completely opposite.\n",
    "\n",
    "By using 1 - cosine, we transform the scale:\n",
    "If the cosine similarity is 1 (vectors are identical), 1 - 1 becomes 0, indicating no difference.\n",
    "If the cosine similarity is 0 (vectors are orthogonal), 1 - 0 becomes 1, indicating maximum difference.\n",
    "A cosine similarity of -1 (completely opposite vectors) would result in a transformed similarity of 2, which typically doesn't occur in normalized vector spaces used in text analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece9a46f291c534d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")  # Make sure you've loaded the original dataset into 'df'\n",
    "df_anonymized = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Modify the get_embedding function to send inputs to the GPU\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2, tokenizer, model):\n",
    "    emb1 = get_embedding(text1, tokenizer, model)\n",
    "    emb2 = get_embedding(text2, tokenizer, model)\n",
    "    # Ensure embeddings are 1-D\n",
    "    emb1 = np.squeeze(emb1)\n",
    "    emb2 = np.squeeze(emb2)\n",
    "    #print(text1 + \" and \" + text2)\n",
    "    return 1 - cosine(emb1, emb2)\n",
    "\n",
    "# Calculate similarities\n",
    "try:\n",
    "    similarity_scores = [semantic_similarity(orig, anon, tokenizer, model) for orig, anon in zip(df['content'], df_anonymized['content'])]\n",
    "except ValueError as e:\n",
    "    print(f\"Error calculating similarity: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-28T14:10:43.724368Z"
    }
   },
   "id": "f016ef0d4f4587e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
