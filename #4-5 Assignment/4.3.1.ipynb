{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #4-5: Anonymising Textual Data and De-Anonymisation\n",
    "- Dataset:  Tweets Emotions [Dataset](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download)\n",
    "- Credits: Dataset was put together by Pashipatu Gupta\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e766beca369aa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Textual Data Anonymisation – 30 marks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c1a5b1a1bad415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Do some research to determine what needs to be anonymised in the data and why.\n",
    "- For a better understanding of the structure of the dataset , we display the attribute values\n",
    "    - What columns does the dataset contain and in what format are the attribute values?\n",
    "        - Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "237a8e23efc1b84d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")\n",
    "print(df.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T09:34:52.303992Z",
     "start_time": "2024-01-27T09:34:51.595231Z"
    }
   },
   "id": "74a9cb538af3b384"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By inspecting the different columns and the data format, the 'content' attribute definitely has the potential to contain explicit personally identifiable information:\n",
    "1. User Mentions: \n",
    "    - Any instance of @username should be anonymised because it directly points to an individual's account, which is considered personally identifiable information (PII).\n",
    "2. First Names: \n",
    "    - If any first names are used in a context that can identify an individual, such as tagging in combination with other identifying information, they should be anonymised.\n",
    "3. Locations and Specific References: \n",
    "    - Any mention of specific locations, addresses, landmarks, or establishments that could help in identifying an individual should be anonymised.\n",
    "4. Specific Events with Identifiable Information: \n",
    "    - References to specific events that may lead to the identification of individuals, like parties or gatherings with a list of names, should be anonymised.\n",
    "5. Unique Identifiers: \n",
    "    - Any other unique identifiers, such as specific dates, times, or unique events, that could potentially be linked back to an individual.\n",
    "\n",
    "\n",
    "Apart from that, the 'sentiment' attribute is explored further as we don't know by now how many unique values there actually are and if they would qualify as PII: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190ed3635d49ace1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of the dataframe:  40000\n",
      "number of unique values in sentiment:  13\n",
      "counts per unique value in sentiment\n",
      "neutral       8638\n",
      "worry         8459\n",
      "happiness     5209\n",
      "sadness       5165\n",
      "love          3842\n",
      "surprise      2187\n",
      "fun           1776\n",
      "relief        1526\n",
      "hate          1323\n",
      "empty          827\n",
      "enthusiasm     759\n",
      "boredom        179\n",
      "anger          110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"total lenth of the dataframe: \", len(df))\n",
    "\n",
    "# Calculate the number of unique values and the number of entries per unique value\n",
    "unique_counts = df['sentiment'].nunique()\n",
    "value_counts = df['sentiment'].value_counts()\n",
    "\n",
    "print(\"number of unique values in sentiment: \", unique_counts)\n",
    "print(\"counts per unique value in\", value_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T09:34:52.309855Z",
     "start_time": "2024-01-27T09:34:52.300603Z"
    }
   },
   "id": "7409df0e76827242"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By inspecting the 'sentiment' attribute further, we can say that there are 13 different values in the 'sentiment' column. We also know that there are 40,000 tweets in total in the dataset. Given this information, there is no need to anonymize the 'sentiment' attribute."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48910199218c1788"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Using a Natural Language Processing library (e.g. Python’s spaCy), analyse the text to identify elements of personally identifiable information (PII).\n",
    "The goal of anonymization is to remove or obscure such details so that the individuals to whom the data pertains cannot be readily identified. The first step is finding the contents, that might actually contain PII.\n",
    "As the first step, we install 'en_core_web_sm', a pre-trained spaCy model suitable for identifying named entities, which include PII. 'en_core_web_sm' is the English model trained on web text. It has been trained on a diverse range of web text, including blogs, news, comments. We've decided on using 'en_core_web_sm' instead of for example 'en_core_web_trf' due to their balance between performance and resource usage.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed5640c245c74b6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm#install model without outputting in console"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T09:34:52.311047Z",
     "start_time": "2024-01-27T09:34:52.308380Z"
    }
   },
   "id": "2c37a5979e8eebe7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julius/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to identify PII using spaCy\n",
    "def identify_pii(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    pii_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pii_entities\n",
    "\n",
    "pii_original = df['content'].apply(identify_pii)\n",
    "\n",
    "df['PII'] = pii_original\n",
    "df.to_csv(\"PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T09:37:38.279020Z",
     "start_time": "2024-01-27T09:34:52.312290Z"
    }
   },
   "id": "7aeb8f4d70fdbdfd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
      "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
      "\n",
      "                        PII  \n",
      "0  [('@tiffanylue', 'ORG')]  \n",
      "1                        []  \n",
      "2      [('friday', 'DATE')]  \n",
      "3                        []  \n",
      "4      [('Houston', 'GPE')]  \n"
     ]
    }
   ],
   "source": [
    "#load new dataframe containing the PII information\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")\n",
    "print(df.iloc[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T09:37:38.345082Z",
     "start_time": "2024-01-27T09:37:38.278954Z"
    }
   },
   "id": "b01646ec40fd6a5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each non-empty list within the square brackets [] in the new 'PII' column indicates that the spaCy model has identified text segments in that specific row which it believes to be named entities. The entities are tagged with labels that classify what type of entity they are (e.g., DATE, PERSON, ORG(=organization), GPE(=Geopolitical Entity)). These named entities can be considered PII, as they might be used to identify an individual either directly or when combined with other additional information. In conclusion, when there is a non-empty list in the 'PII' column in a specific row, we have to apply some sort of anonymisation mechanism to prevent the PIIs from being able to identify an individual."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123e44b1e28173f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d5e33a6d5b2a6549"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Using the techniques you applied in Assignment #1, apply a masking or transformation mechanism to modify the detected PII elements and substitute with suitable replacements.\n",
    "In the following section, we will apply techniques similar to those used in Assignment #1 to mask or transform Personally Identifiable Information (PII) detected in a dataset. The goal is to substitute these sensitive elements with suitable replacements while maintaining the overall structure and coherence of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4165656a106a2752"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a first step, we start by checking how many occurences of which category we have in our new PII column. This information is crucial for planning further anonymization steps. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7140d6e704d3e823"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PERSON': 8403, 'ORG': 7517, 'DATE': 7360, 'CARDINAL': 3814, 'GPE': 3264, 'TIME': 2949, 'NORP': 840, 'ORDINAL': 731, 'PRODUCT': 400, 'MONEY': 335, 'WORK_OF_ART': 335, 'LOC': 291, 'FAC': 190, 'QUANTITY': 182, 'EVENT': 79, 'PERCENT': 71, 'LANGUAGE': 65, 'LAW': 29})\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import ast \n",
    "\n",
    "# Function to extract entities from a string and return their labels\n",
    "def extract_labels(data_string):\n",
    "    # Convert string representation of list to actual list\n",
    "    entities = ast.literal_eval(data_string)\n",
    "    # Extract labels\n",
    "    return [label for _, label in entities]\n",
    "\n",
    "# Extract labels from each item in the data\n",
    "all_labels = [label for item in df['PII'] for label in extract_labels(item)]\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = collections.Counter(all_labels)\n",
    "\n",
    "print(label_counts)\n",
    "print(len(label_counts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T10:02:18.299318Z",
     "start_time": "2024-01-27T10:02:18.035540Z"
    }
   },
   "id": "1187a80fac783715"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we now know that there are 18 types of different datatypes that should be anonymized in some kind of way. We start by anonymizing the easiest ones with the faker library:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "850b475e50dc1c9d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T10:04:47.987236Z",
     "start_time": "2024-01-27T10:04:47.932916Z"
    }
   },
   "id": "21ba01f299ad3f70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import re\n",
    "\n",
    "fake = Faker()\n",
    "def replace_pii_with_fake(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    # Iterate over the identified entities\n",
    "    for ent in doc.ents:\n",
    "        # Replace with fake data based on the entity type\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = re.sub(re.escape(ent.text), fake.name(), text)\n",
    "        elif ent.label_ == 'GPE':\n",
    "            text = re.sub(re.escape(ent.text), fake.city(), text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            text = re.sub(re.escape(ent.text), fake.date(), text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            text = re.sub(re.escape(ent.text), fake.company(), text)\n",
    "        elif ent.label_ == 'NORP':\n",
    "            text = re.sub(re.escape(ent.text), fake.country(), text)\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            text = re.sub(re.escape(ent.text), str(fake.random_number()), text)\n",
    "        elif ent.label_ == 'TIME':\n",
    "            text = re.sub(re.escape(ent.text), fake.time(), text)\n",
    "            \n",
    "    # Replace Twitter @ with fake names\n",
    "    text = re.sub(r'(?<=@)\\w+', fake.user_name(), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(replace_pii_with_fake)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-27T10:06:44.268900Z"
    }
   },
   "id": "7aef841f332cbc5"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7693ca64a9ec2b84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Analyse the text to determine if any information can be obtained after the transformation process. What conclusions can you draw from this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb3962248019abc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m pii_labels \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPERSON\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPE\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDATE\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mORG\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNORP\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Filter the entities by the labels of interest\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m pii_original_filtered \u001B[38;5;241m=\u001B[39m [(text, label) \u001B[38;5;28;01mfor\u001B[39;00m text, label \u001B[38;5;129;01min\u001B[39;00m pii_original \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m pii_labels]\n\u001B[1;32m     11\u001B[0m pii_anonymised_filtered \u001B[38;5;241m=\u001B[39m [(text, label) \u001B[38;5;28;01mfor\u001B[39;00m text, label \u001B[38;5;129;01min\u001B[39;00m pii_anonymised \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m pii_labels]\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Convert lists to sets\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[7], line 10\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      7\u001B[0m pii_labels \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPERSON\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPE\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDATE\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mORG\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNORP\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Filter the entities by the labels of interest\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m pii_original_filtered \u001B[38;5;241m=\u001B[39m [(text, label) \u001B[38;5;28;01mfor\u001B[39;00m text, label \u001B[38;5;129;01min\u001B[39;00m pii_original \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m pii_labels]\n\u001B[1;32m     11\u001B[0m pii_anonymised_filtered \u001B[38;5;241m=\u001B[39m [(text, label) \u001B[38;5;28;01mfor\u001B[39;00m text, label \u001B[38;5;129;01min\u001B[39;00m pii_anonymised \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m pii_labels]\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Convert lists to sets\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "df_anonymized = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "\n",
    "pii_anonymised = df_anonymized['content'].apply(identify_pii)\n",
    "\n",
    "\n",
    "# Define the PII labels of interest\n",
    "pii_labels = {'PERSON', 'GPE', 'DATE', 'ORG', 'NORP'}\n",
    "\n",
    "# Filter the entities by the labels of interest\n",
    "pii_original_filtered = [(text, label) for text, label in pii_original if label in pii_labels]\n",
    "pii_anonymised_filtered = [(text, label) for text, label in pii_anonymised if label in pii_labels]\n",
    "\n",
    "# Convert lists to sets\n",
    "set_pii_original = set(pii_original_filtered)\n",
    "set_pii_anonymised = set(pii_anonymised_filtered)\n",
    "\n",
    "# Find common elements\n",
    "common_pii = set_pii_original.intersection(set_pii_anonymised)\n",
    "\n",
    "# Check if there are any common elements\n",
    "if len(common_pii) > 0:\n",
    "    print(\"The lists have common PII values:\", common_pii)\n",
    "else:\n",
    "    print(\"The lists do not have any common PII values.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T09:43:02.389884Z",
     "start_time": "2024-01-27T09:40:18.175516Z"
    }
   },
   "id": "bbb01468fc7c2b29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T09:43:02.391112Z"
    }
   },
   "id": "7fff0bd4d4f17ea3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T09:43:02.392266Z"
    }
   },
   "id": "6be6a4501e271270"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
