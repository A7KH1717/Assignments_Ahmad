{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #4-5: Anonymising Textual Data and De-Anonymisation\n",
    "- Dataset:  Tweets Emotions [Dataset](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download)\n",
    "- Credits: Dataset was put together by Pashipatu Gupta\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e766beca369aa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Textual Data Anonymisation – 30 marks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c1a5b1a1bad415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Do some research to determine what needs to be anonymised in the data and why.\n",
    "- For a better understanding of the structure of the dataset , we display the attribute values\n",
    "    - What columns does the dataset contain and in what format are the attribute values?\n",
    "        - Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "237a8e23efc1b84d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")\n",
    "print(df.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:04:52.629841700Z",
     "start_time": "2024-01-27T13:04:52.561553100Z"
    }
   },
   "id": "74a9cb538af3b384"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By inspecting the different columns and the data format, the 'content' attribute definitely has the potential to contain explicit personally identifiable information:\n",
    "1. User Mentions: \n",
    "    - Any instance of @username should be anonymised because it directly points to an individual's account, which is considered personally identifiable information (PII).\n",
    "2. First Names: \n",
    "    - If any first names are used in a context that can identify an individual, such as tagging in combination with other identifying information, they should be anonymised.\n",
    "3. Locations and Specific References: \n",
    "    - Any mention of specific locations, addresses, landmarks, or establishments that could help in identifying an individual should be anonymised.\n",
    "4. Specific Events with Identifiable Information: \n",
    "    - References to specific events that may lead to the identification of individuals, like parties or gatherings with a list of names, should be anonymised.\n",
    "5. Unique Identifiers: \n",
    "    - Any other unique identifiers, such as specific dates, times, or unique events, that could potentially be linked back to an individual.\n",
    "\n",
    "\n",
    "Apart from that, the 'sentiment' attribute is explored further as we don't know by now how many unique values there actually are and if they would qualify as PII: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190ed3635d49ace1"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of the dataframe:  40000\n",
      "number of unique values in sentiment:  13\n",
      "counts per unique value in sentiment\n",
      "neutral       8638\n",
      "worry         8459\n",
      "happiness     5209\n",
      "sadness       5165\n",
      "love          3842\n",
      "surprise      2187\n",
      "fun           1776\n",
      "relief        1526\n",
      "hate          1323\n",
      "empty          827\n",
      "enthusiasm     759\n",
      "boredom        179\n",
      "anger          110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"total lenth of the dataframe: \", len(df))\n",
    "\n",
    "# Calculate the number of unique values and the number of entries per unique value\n",
    "unique_counts = df['sentiment'].nunique()\n",
    "value_counts = df['sentiment'].value_counts()\n",
    "\n",
    "print(\"number of unique values in sentiment: \", unique_counts)\n",
    "print(\"counts per unique value in\", value_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:04:52.630840900Z",
     "start_time": "2024-01-27T13:04:52.621713400Z"
    }
   },
   "id": "7409df0e76827242"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By inspecting the 'sentiment' attribute further, we can say that there are 13 different values in the 'sentiment' column. We also know that there are 40,000 tweets in total in the dataset. Given this information, there is no need to anonymize the 'sentiment' attribute."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48910199218c1788"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Using a Natural Language Processing library (e.g. Python’s spaCy), analyse the text to identify elements of personally identifiable information (PII).\n",
    "The goal of anonymization is to remove or obscure such details so that the individuals to whom the data pertains cannot be readily identified. The first step is finding the contents, that might actually contain PII.\n",
    "As the first step, we install 'en_core_web_lg', a pre-trained spaCy model suitable for identifying named entities, which include PII. 'en_core_web_lg' is the English model trained on web text. It has been trained on a diverse range of web text, including blogs, news, comments. We've decided on using 'en_core_web_lg' instead of for example 'en_core_web_trf' due to their balance between performance and resource usage.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed5640c245c74b6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg > /dev/null 2>&1 #install model without outputting in console"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:04:52.695524600Z",
     "start_time": "2024-01-27T13:04:52.628679200Z"
    }
   },
   "id": "2c37a5979e8eebe7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Function to identify PII using spaCy\n",
    "def identify_pii(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    pii_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pii_entities\n",
    "\n",
    "pii_original = df['content'].apply(identify_pii)\n",
    "\n",
    "df['PII'] = pii_original\n",
    "df.to_csv(\"PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:07:45.899015200Z",
     "start_time": "2024-01-27T13:04:52.640313600Z"
    }
   },
   "id": "7aeb8f4d70fdbdfd"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
      "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
      "\n",
      "                           PII  \n",
      "0  [('@tiffanylue', 'PERSON')]  \n",
      "1                           []  \n",
      "2         [('friday', 'DATE')]  \n",
      "3                           []  \n",
      "4         [('Houston', 'GPE')]  \n"
     ]
    }
   ],
   "source": [
    "#load new dataframe containing the PII information\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")\n",
    "print(df.iloc[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:07:45.974302700Z",
     "start_time": "2024-01-27T13:07:45.900222800Z"
    }
   },
   "id": "b01646ec40fd6a5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each non-empty list within the square brackets [] in the new 'PII' column indicates that the spaCy model has identified text segments in that specific row which it believes to be named entities. The entities are tagged with labels that classify what type of entity they are (e.g., DATE, PERSON, ORG(=organization), GPE(=Geopolitical Entity)). These named entities can be considered PII, as they might be used to identify an individual either directly or when combined with other additional information. In conclusion, when there is a non-empty list in the 'PII' column in a specific row, we have to apply some sort of anonymisation mechanism to prevent the PIIs from being able to identify an individual."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123e44b1e28173f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Using the techniques you applied in Assignment #1, apply a masking or transformation mechanism to modify the detected PII elements and substitute with suitable replacements.\n",
    "In the following section, we will apply techniques similar to those used in Assignment #1 to mask or transform Personally Identifiable Information (PII) detected in a dataset. The goal is to substitute these sensitive elements with suitable replacements while maintaining the overall structure and coherence of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4165656a106a2752"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a first step, we start by checking how many occurrences of which category we have in our new PII column. This information is crucial for planning further anonymization steps. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7140d6e704d3e823"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PERSON': 10340, 'ORG': 8815, 'DATE': 7560, 'CARDINAL': 3430, 'GPE': 3269, 'TIME': 2964, 'NORP': 957, 'PRODUCT': 702, 'ORDINAL': 680, 'WORK_OF_ART': 607, 'MONEY': 353, 'LOC': 244, 'FAC': 210, 'QUANTITY': 191, 'EVENT': 123, 'LANGUAGE': 106, 'PERCENT': 75, 'LAW': 35})\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import ast \n",
    "\n",
    "# Function to extract entities from a string and return their labels\n",
    "def extract_labels(data_string):\n",
    "    # Convert string representation of list to actual list\n",
    "    entities = ast.literal_eval(data_string)\n",
    "    # Extract labels\n",
    "    return [label for _, label in entities]\n",
    "\n",
    "# Extract labels from each item in the data\n",
    "all_labels = [label for item in df['PII'] for label in extract_labels(item)]\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = collections.Counter(all_labels)\n",
    "\n",
    "print(label_counts)\n",
    "print(len(label_counts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:07:46.226409900Z",
     "start_time": "2024-01-27T13:07:46.003198300Z"
    }
   },
   "id": "1187a80fac783715"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 18 different categories have the following meaning: \n",
    "\n",
    "- PERSON: Names of people.\n",
    "- ORG: Organizations, including companies, agencies, institutions, etc.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- CARDINAL: Numerals that do not fall under another type (like dates or quantities).\n",
    "- GPE: Geopolitical entity, typically referring to countries, cities, states.\n",
    "- TIME: Times smaller than a day, including specific time periods, durations, or times of day.\n",
    "- NORP: Nationalities, religious or political groups.\n",
    "- ORDINAL: \"First\", \"second\", etc., used to denote position in a ordered sequence.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (not services).\n",
    "- MONEY: Monetary values, including unit.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- FAC: Facilities, including buildings, airports, highways, bridges, etc.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- PERCENT: Percentage (including \"%\").\n",
    "- LANGUAGE: Any named language.\n",
    "- LAW: Named documents made into laws.\n",
    "\n",
    "\n",
    "So, we now know that there are 18 types of different datatypes that should be anonymized in some kind of way. We start by anonymizing the easiest ones with the faker library:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "850b475e50dc1c9d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:07:46.282333900Z",
     "start_time": "2024-01-27T13:07:46.224407800Z"
    }
   },
   "id": "21ba01f299ad3f70"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import re\n",
    "import random\n",
    "\n",
    "def close_number(original_number):\n",
    "    try:\n",
    "        num = int(original_number)\n",
    "        # Generate a number within ±10% of the original number, for example\n",
    "        variation = int(num * 0.1)\n",
    "        return str(random.randint(max(0, num - variation), num + variation))\n",
    "    except ValueError:\n",
    "        return original_number  \n",
    "    \n",
    "def fake_ordinal():\n",
    "    number = fake.random_int(min=1, max=100)\n",
    "    suffix = [\"th\", \"st\", \"nd\", \"rd\"] + [\"th\"] * 6\n",
    "    return str(number) + suffix[number % 10 if number % 100 not in [11, 12, 13] else 0]\n",
    "    \n",
    "fake = Faker()\n",
    "def replace_pii_with_fake(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    # Iterate over the identified entities\n",
    "    for ent in doc.ents: #a\n",
    "        # Replace with fake data based on the entity type\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = re.sub(re.escape(ent.text), fake.name(), text)\n",
    "        elif ent.label_ == 'GPE':\n",
    "            text = re.sub(re.escape(ent.text), fake.city(), text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            text = re.sub(re.escape(ent.text), fake.date(), text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            text = re.sub(re.escape(ent.text), fake.company(), text)\n",
    "        elif ent.label_ == 'NORP':\n",
    "            text = re.sub(re.escape(ent.text), fake.country(), text)\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            text = re.sub(re.escape(ent.text), lambda x: close_number(ent.text), text)\n",
    "        elif ent.label_ == 'ORDINAL':\n",
    "            text = re.sub(re.escape(ent.text), fake_ordinal(), text)\n",
    "        elif ent.label_ == 'TIME':\n",
    "            text = re.sub(re.escape(ent.text), fake.time(), text)\n",
    "            \n",
    "    # Replace Twitter @ with fake names\n",
    "    text = re.sub(r'(?<=@)\\w+', fake.user_name(), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(replace_pii_with_fake)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T13:10:44.906840300Z",
     "start_time": "2024-01-27T13:07:46.285825200Z"
    }
   },
   "id": "7aef841f332cbc5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, that we anonymized seven of the 18 total categories, lets continue with the other 11, that are still missing: \n",
    "- ORDINAL PRODUCT MONEY WORK_OF_ART LOC FAC QUANTITY EVENT PERCENT LANGUAGE LAW\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e29aab4c1bfc9a9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7693ca64a9ec2b84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Analyse the text to determine if any information can be obtained after the transformation process. What conclusions can you draw from this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb3962248019abc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idea is to check semantics of text from original dataset and anonymised dataset and see if they are similar. Then check if the PII from the original dataset are still present in the anonymised dataset\n",
    "\n",
    "Cosine Similarity: Cosine similarity measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. Its value ranges from -1 to 1, where:\n",
    "\n",
    "1 means the vectors are identical.\n",
    "0 indicates orthogonality (no similarity).\n",
    "-1 implies completely opposite.\n",
    "\n",
    "By using 1 - cosine, we transform the scale:\n",
    "If the cosine similarity is 1 (vectors are identical), 1 - 1 becomes 0, indicating no difference.\n",
    "If the cosine similarity is 0 (vectors are orthogonal), 1 - 0 becomes 1, indicating maximum difference.\n",
    "A cosine similarity of -1 (completely opposite vectors) would result in a transformed similarity of 2, which typically doesn't occur in normalized vector spaces used in text analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5a8cdde3161454"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")  # Make sure you've loaded the original dataset into 'df'\n",
    "df_anonymized = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Modify the get_embedding function to send inputs to the GPU\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2, tokenizer, model):\n",
    "    emb1 = get_embedding(text1, tokenizer, model)\n",
    "    emb2 = get_embedding(text2, tokenizer, model)\n",
    "    # Ensure embeddings are 1-D\n",
    "    emb1 = np.squeeze(emb1)\n",
    "    emb2 = np.squeeze(emb2)\n",
    "    #print(text1 + \" and \" + text2)\n",
    "    return 1 - cosine(emb1, emb2)\n",
    "\n",
    "# Calculate similarities\n",
    "try:\n",
    "    similarity_scores = [semantic_similarity(orig, anon, tokenizer, model) for orig, anon in zip(df['content'], df_anonymized['content'])]\n",
    "except ValueError as e:\n",
    "    print(f\"Error calculating similarity: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-27T13:35:52.524449500Z"
    }
   },
   "id": "bbb01468fc7c2b29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T13:11:25.632657500Z"
    }
   },
   "id": "7fff0bd4d4f17ea3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T13:11:25.634782400Z"
    }
   },
   "id": "6be6a4501e271270"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
