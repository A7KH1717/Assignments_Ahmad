{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 De-anonymizing a dataset â€“ 50 marks\n",
    "For this task, we are using a dataset provided to us by our colleagues. The Dataset has been anonymized using bayesian inferences. \n",
    "- Dataset: police-shooting_anonymized.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1dad3c5496335"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Using standard search mechanisms, determine if there are any elements within the dataset that you received, that allow for de-anonymizations to occur. Make a note of what you find and explain the procedure you used. \n",
    "First of all, we look at the data present in our new dataset. The dataset contains information about victims of police shootings with information about where they happened (city, state, longitude, latitude), when they happened (date), information about the victims (gender, race, manner of death, signs_of_mental_illness, threat level, flee) and body_camera giving information about whether the police officer was wearing a body cam. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7324cb9cdbe8e0e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date   manner_of_death       armed   age gender race  \\\n",
      "0  2022-11-06              shot         gun  20.0      M    H   \n",
      "1  2016-01-20              shot         gun  32.0      M    W   \n",
      "2  2020-08-29  shot and Tasered     unarmed  56.0      M    B   \n",
      "3  2020-07-12              shot  toy weapon  52.0      F    B   \n",
      "4  2019-04-29              shot    nail gun  19.0      M    B   \n",
      "\n",
      "               city state  signs_of_mental_illness threat_level         flee  \\\n",
      "0         Baltimore    AZ                     True       attack  Not fleeing   \n",
      "1         Kerrville    ME                    False       attack  Not fleeing   \n",
      "2  Dearborn Heights    NC                    False        other  Not fleeing   \n",
      "3   Butler Township    FL                     True       attack  Not fleeing   \n",
      "4            Edmond    MS                    False       attack  Not fleeing   \n",
      "\n",
      "   body_camera  longitude  latitude  is_geocoding_exact  \n",
      "0        False    -83.663    37.472                True  \n",
      "1        False    -86.043    38.879                True  \n",
      "2        False    -96.642    32.958                True  \n",
      "3        False    -82.765    28.150                True  \n",
      "4        False    -93.746    32.492                True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"police_shooting_anonymized.csv\")\n",
    "print(df.iloc[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T17:47:26.623811Z",
     "start_time": "2024-02-20T17:47:26.588159Z"
    }
   },
   "id": "97a2532da23b39bb",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "dIn our case, we tried google searching some of the entries on the internet trying to match information with the given dataset. The first entry of the dataset directly provides us with multiple online articles talking about the shooting in Baltimore on the 06.11.2022 without mentioning the name of the victim. We can find the respective name after searching for another while in a YouTube video showing the body cam of an officer present at the shooting on the 06.11.2022 - The name of the victim was Tyree Moorehead. \n",
    "Going further we tried matching other entries to real world events and there wasn't one entry we did not succeed with a simple google search. For the first five entries, the victim names are: \n",
    "- Tyree Moorehead\n",
    "- Michael Clyde Lynch\n",
    "- Donny Walker\n",
    "- Terena Nicole Thurman\n",
    "- Isaiah Lewis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbd8602ae9d4c0d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Design a de-anonymisation algorithm and apply to both the received dataset and your dataset. Report on the following:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c3e2ef7e7a435a4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier rows with distances have been exported to 'outliers_with_distances.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset from CSV file\n",
    "file_path = 'police_shooting_anonymized.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'date' column to a numerical feature\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['days_since'] = (df['date'] - df['date'].min()).dt.days\n",
    "\n",
    "# Select columns to be one-hot encoded\n",
    "categorical_cols = ['city', 'manner_of_death']\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoded_features = encoder.fit_transform(df[categorical_cols]).toarray()\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "# Combine encoded features with the numerical 'days_since' column\n",
    "final_df = pd.concat([encoded_df, df['days_since']], axis=1)\n",
    "\n",
    "# PCA and StandardScaler in a Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2))  # Adjust n_components as needed\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "pca_features = pipeline.fit_transform(final_df)\n",
    "\n",
    "# Calculate the Euclidean distance from the origin for each point\n",
    "distances = np.linalg.norm(pca_features, axis=1)\n",
    "\n",
    "# Determine a threshold for outliers\n",
    "threshold = np.mean(distances) + 2 * np.std(distances)\n",
    "\n",
    "# Identify outliers\n",
    "outliers = distances > threshold\n",
    "\n",
    "# Retrieve the entire corresponding rows of the dataset for the outliers\n",
    "outlier_indices = np.where(outliers)[0]\n",
    "outlier_rows = df.iloc[outlier_indices].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Add the outlier distances to these rows\n",
    "outlier_rows['outlier_distance'] = distances[outliers]\n",
    "\n",
    "# Export the outlier rows with distances to a new CSV file\n",
    "outlier_rows.to_csv('outliers_with_distances.csv', index=False)\n",
    "\n",
    "print(\"Outlier rows with distances have been exported to 'outliers_with_distances.csv'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T17:47:27.364355Z",
     "start_time": "2024-02-20T17:47:26.612955Z"
    }
   },
   "id": "46b12fa7233d4e62",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T17:47:27.364987Z",
     "start_time": "2024-02-20T17:47:27.360185Z"
    }
   },
   "id": "a61c17fae7e2ea3d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
