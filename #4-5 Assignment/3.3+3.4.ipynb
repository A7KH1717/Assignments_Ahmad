{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Using the techniques you applied in Assignment #1, apply a masking or transformation mechanism to modify the detected PII elements and substitute with suitable replacements.\n",
    "In the following section, we will apply techniques similar to those used in Assignment #1 to mask or transform Personally Identifiable Information (PII) detected in a dataset. The goal is to substitute these sensitive elements with suitable replacements while maintaining the overall structure and coherence of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf64464ca6c1a3f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a first step, we start by checking how many occurrences of which category we have in our new PII column. This information is crucial for planning further anonymization steps. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7817bcdb4dd409c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:22:54.359228Z",
     "start_time": "2024-01-27T18:22:54.298326Z"
    }
   },
   "id": "e43fe2f2f918c4e8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PERSON': 10340, 'ORG': 8815, 'DATE': 7560, 'CARDINAL': 3430, 'GPE': 3269, 'TIME': 2964, 'NORP': 957, 'PRODUCT': 702, 'ORDINAL': 680, 'WORK_OF_ART': 607, 'MONEY': 353, 'LOC': 244, 'FAC': 210, 'QUANTITY': 191, 'EVENT': 123, 'LANGUAGE': 106, 'PERCENT': 75, 'LAW': 35})\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import ast \n",
    "\n",
    "# Function to extract entities from a string and return their labels\n",
    "def extract_labels(data_string):\n",
    "    # Convert string representation of list to actual list\n",
    "    entities = ast.literal_eval(data_string)\n",
    "    # Extract labels\n",
    "    return [label for _, label in entities]\n",
    "\n",
    "# Extract labels from each item in the data\n",
    "all_labels = [label for item in df['PII'] for label in extract_labels(item)]\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = collections.Counter(all_labels)\n",
    "\n",
    "print(label_counts)\n",
    "print(len(label_counts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:22:54.641773Z",
     "start_time": "2024-01-27T18:22:54.394203Z"
    }
   },
   "id": "ca1a2b35fc634953"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 18 different categories have the following meaning: \n",
    "\n",
    "- PERSON: Names of people.\n",
    "- ORG: Organizations, including companies, agencies, institutions, etc.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- CARDINAL: Numerals that do not fall under another type (like dates or quantities).\n",
    "- GPE: Geopolitical entity, typically referring to countries, cities, states.\n",
    "- TIME: Times smaller than a day, including specific time periods, durations, or times of day.\n",
    "- NORP: Nationalities, religious or political groups.\n",
    "- ORDINAL: \"First\", \"second\", etc., used to denote position in a ordered sequence.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (not services).\n",
    "- MONEY: Monetary values, including unit.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- FAC: Facilities, including buildings, airports, highways, bridges, etc.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- PERCENT: Percentage (including \"%\").\n",
    "- LANGUAGE: Any named language.\n",
    "- LAW: Named documents made into laws.\n",
    "\n",
    "\n",
    "So, we now know that there are 18 types of different datatypes that should be anonymized in some kind of way. We start by anonymizing the easiest ones with the faker library:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba51d728bb1e8400"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julius/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Load the large, pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:22:57.820355Z",
     "start_time": "2024-01-27T18:22:54.640256Z"
    }
   },
   "id": "b05743fe8a77ffb9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import re\n",
    "import random\n",
    "\n",
    "def close_number(original_number):\n",
    "    try:\n",
    "        num = int(original_number)\n",
    "        # Generate a number within Â±10% of the original number, for example\n",
    "        variation = int(num * 0.1)\n",
    "        return str(random.randint(max(0, num - variation), num + variation))\n",
    "    except ValueError:\n",
    "        return original_number  \n",
    "    \n",
    "def fake_ordinal():\n",
    "    number = fake.random_int(min=1, max=100)\n",
    "    suffix = [\"th\", \"st\", \"nd\", \"rd\"] + [\"th\"] * 6\n",
    "    return str(number) + suffix[number % 10 if number % 100 not in [11, 12, 13] else 0]\n",
    "    \n",
    "fake = Faker()\n",
    "def replace_pii_with_fake(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    # Iterate over the identified entities\n",
    "    for ent in doc.ents: #a\n",
    "        # Replace with fake data based on the entity type\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = re.sub(re.escape(ent.text), fake.name(), text)\n",
    "        elif ent.label_ == 'GPE':\n",
    "            text = re.sub(re.escape(ent.text), fake.city(), text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            text = re.sub(re.escape(ent.text), fake.date(), text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            text = re.sub(re.escape(ent.text), fake.company(), text)\n",
    "        elif ent.label_ == 'NORP':\n",
    "            text = re.sub(re.escape(ent.text), fake.country(), text)\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            text = re.sub(re.escape(ent.text), lambda x: close_number(ent.text), text)\n",
    "        elif ent.label_ == 'ORDINAL':\n",
    "            text = re.sub(re.escape(ent.text), fake_ordinal(), text)\n",
    "        elif ent.label_ == 'TIME':\n",
    "            text = re.sub(re.escape(ent.text), fake.time(), text)\n",
    "            \n",
    "    # Replace Twitter @ with fake names\n",
    "    text = re.sub(r'(?<=@)\\w+', fake.user_name(), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(replace_pii_with_fake)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:26:08.211130Z",
     "start_time": "2024-01-27T18:22:57.826562Z"
    }
   },
   "id": "3e67063453962912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, that we anonymized seven of the 18 total categories, lets continue with the other 11, that are still missing: \n",
    "- ORDINAL PRODUCT MONEY WORK_OF_ART LOC FAC QUANTITY EVENT PERCENT LANGUAGE LAW\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f04675cea0f130f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:26:08.212254Z",
     "start_time": "2024-01-27T18:26:08.210243Z"
    }
   },
   "id": "3517b896a417891d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Analyse the text to determine if any information can be obtained after the transformation process. What conclusions can you draw from this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d24d03b52d742b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idea is to check semantics of text from original dataset and anonymised dataset and see if they are similar. Then check if the PII from the original dataset are still present in the anonymised dataset\n",
    "\n",
    "Cosine Similarity: Cosine similarity measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. Its value ranges from -1 to 1, where:\n",
    "\n",
    "1 means the vectors are identical.\n",
    "0 indicates orthogonality (no similarity).\n",
    "-1 implies completely opposite.\n",
    "\n",
    "By using 1 - cosine, we transform the scale:\n",
    "If the cosine similarity is 1 (vectors are identical), 1 - 1 becomes 0, indicating no difference.\n",
    "If the cosine similarity is 0 (vectors are orthogonal), 1 - 0 becomes 1, indicating maximum difference.\n",
    "A cosine similarity of -1 (completely opposite vectors) would result in a transformed similarity of 2, which typically doesn't occur in normalized vector spaces used in text analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece9a46f291c534d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julius/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/julius/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/special/_ufuncs.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_npy_asinh'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistance\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cosine\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DistilBertTokenizer, DistilBertModel\n",
      "File \u001B[0;32m~/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/spatial/__init__.py:111\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plotutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_procrustes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m procrustes\n\u001B[0;32m--> 111\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_geometric_slerp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m geometric_slerp\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ckdtree, kdtree, qhull\n",
      "File \u001B[0;32m~/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/spatial/_geometric_slerp.py:9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistance\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m euclidean\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnpt\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:121\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _hausdorff\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m norm\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspecial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rel_entr\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _distance_pybind\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _deprecated\n",
      "File \u001B[0;32m~/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/special/__init__.py:649\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m========================================\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mSpecial functions (:mod:`scipy.special`)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    644\u001B[0m \n\u001B[1;32m    645\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    647\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_sf_error\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SpecialFunctionWarning, SpecialFunctionError\n\u001B[0;32m--> 649\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _ufuncs\n\u001B[1;32m    650\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ufuncs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _basic\n",
      "\u001B[0;31mImportError\u001B[0m: dlopen(/Users/julius/PycharmProjects/PPOD/.venv/lib/python3.9/site-packages/scipy/special/_ufuncs.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_npy_asinh'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")  # Make sure you've loaded the original dataset into 'df'\n",
    "df_anonymized = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Modify the get_embedding function to send inputs to the GPU\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2, tokenizer, model):\n",
    "    emb1 = get_embedding(text1, tokenizer, model)\n",
    "    emb2 = get_embedding(text2, tokenizer, model)\n",
    "    # Ensure embeddings are 1-D\n",
    "    emb1 = np.squeeze(emb1)\n",
    "    emb2 = np.squeeze(emb2)\n",
    "    #print(text1 + \" and \" + text2)\n",
    "    return 1 - cosine(emb1, emb2)\n",
    "\n",
    "# Calculate similarities\n",
    "try:\n",
    "    similarity_scores = [semantic_similarity(orig, anon, tokenizer, model) for orig, anon in zip(df['content'], df_anonymized['content'])]\n",
    "except ValueError as e:\n",
    "    print(f\"Error calculating similarity: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:26:09.746146Z",
     "start_time": "2024-01-27T18:26:08.216713Z"
    }
   },
   "id": "f016ef0d4f4587e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbformat import validate\n",
    "\n",
    "# Replace '4.3.1.ipynb' with the path to your notebook\n",
    "with open('3.1+3.2.ipynb', 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "    validate(nb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T18:26:09.747823Z"
    }
   },
   "id": "155123d4fa28d63d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T18:26:09.748914Z"
    }
   },
   "id": "3f5e3874d855f78f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
