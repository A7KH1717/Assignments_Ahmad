{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Using the techniques you applied in Assignment #1, apply a masking or transformation mechanism to modify the detected PII elements and substitute with suitable replacements.\n",
    "In the following section, we will apply techniques similar to those used in Assignment #1 to mask or transform Personally Identifiable Information (PII) detected in a dataset. The goal is to substitute these sensitive elements with suitable replacements while maintaining the overall structure and coherence of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf64464ca6c1a3f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a first step, we start by checking how many occurrences of which category we have in our new PII column. This information is crucial for planning further anonymization steps. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7817bcdb4dd409c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T20:04:04.838524200Z",
     "start_time": "2024-01-27T20:04:04.770960500Z"
    }
   },
   "id": "e43fe2f2f918c4e8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PERSON': 10340, 'ORG': 8815, 'DATE': 7560, 'CARDINAL': 3430, 'GPE': 3269, 'TIME': 2964, 'NORP': 957, 'PRODUCT': 702, 'ORDINAL': 680, 'WORK_OF_ART': 607, 'MONEY': 353, 'LOC': 244, 'FAC': 210, 'QUANTITY': 191, 'EVENT': 123, 'LANGUAGE': 106, 'PERCENT': 75, 'LAW': 35})\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import ast \n",
    "\n",
    "# Function to extract entities from a string and return their labels\n",
    "def extract_labels(data_string):\n",
    "    # Convert string representation of list to actual list\n",
    "    entities = ast.literal_eval(data_string)\n",
    "    # Extract labels\n",
    "    return [label for _, label in entities]\n",
    "\n",
    "# Extract labels from each item in the data\n",
    "all_labels = [label for item in df['PII'] for label in extract_labels(item)]\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = collections.Counter(all_labels)\n",
    "\n",
    "print(label_counts)\n",
    "print(len(label_counts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T20:04:05.238983900Z",
     "start_time": "2024-01-27T20:04:04.841575400Z"
    }
   },
   "id": "ca1a2b35fc634953"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 18 different categories have the following meaning: \n",
    "\n",
    "- PERSON: Names of people.\n",
    "- ORG: Organizations, including companies, agencies, institutions, etc.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- CARDINAL: Numerals that do not fall under another type (like dates or quantities).\n",
    "- GPE: Geopolitical entity, typically referring to countries, cities, states.\n",
    "- TIME: Times smaller than a day, including specific time periods, durations, or times of day.\n",
    "- NORP: Nationalities, religious or political groups.\n",
    "- ORDINAL: \"First\", \"second\", etc., used to denote position in a ordered sequence.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (not services).\n",
    "- MONEY: Monetary values, including unit.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- FAC: Facilities, including buildings, airports, highways, bridges, etc.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- PERCENT: Percentage (including \"%\").\n",
    "- LANGUAGE: Any named language.\n",
    "- LAW: Named documents made into laws.\n",
    "\n",
    "\n",
    "So, we now know that there are 18 types of different datatypes that should be anonymized in some kind of way. We start by anonymizing the easiest ones with the faker library:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba51d728bb1e8400"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Load the large, pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T20:04:06.568876700Z",
     "start_time": "2024-01-27T20:04:05.233952500Z"
    }
   },
   "id": "b05743fe8a77ffb9"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '#'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 57\u001B[0m\n\u001B[0;32m     53\u001B[0m             text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(re\u001B[38;5;241m.\u001B[39mescape(ent\u001B[38;5;241m.\u001B[39mtext), fake\u001B[38;5;241m.\u001B[39mtime(), text)\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[1;32m---> 57\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplace_pii_with_fake\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m# Save the modified DataFrame to a new CSV file\u001B[39;00m\n\u001B[0;32m     60\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnonymized_PII_tweet_emotions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPODNew\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4904\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4769\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4770\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4771\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4776\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4777\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4778\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4779\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4780\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4895\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4896\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4897\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4898\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4900\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4901\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4902\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4903\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4904\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPODNew\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPODNew\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPODNew\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPODNew\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[18], line 49\u001B[0m, in \u001B[0;36mreplace_pii_with_fake\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     47\u001B[0m     text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(re\u001B[38;5;241m.\u001B[39mescape(ent\u001B[38;5;241m.\u001B[39mtext), fake\u001B[38;5;241m.\u001B[39mcountry(), text)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m ent\u001B[38;5;241m.\u001B[39mlabel_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCARDINAL\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 49\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mescape\u001B[49m\u001B[43m(\u001B[49m\u001B[43ment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43ment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m ent\u001B[38;5;241m.\u001B[39mlabel_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mORDINAL\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     51\u001B[0m     text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(re\u001B[38;5;241m.\u001B[39mescape(ent\u001B[38;5;241m.\u001B[39mtext), fake_ordinal(), text)\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\re\\__init__.py:185\u001B[0m, in \u001B[0;36msub\u001B[1;34m(pattern, repl, string, count, flags)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msub\u001B[39m(pattern, repl, string, count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    179\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001B[39;00m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001B[39;00m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001B[39;00m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m    a callable, it's passed the Match object and must return\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;124;03m    a replacement string to be used.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[18], line 49\u001B[0m, in \u001B[0;36mreplace_pii_with_fake.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     47\u001B[0m     text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(re\u001B[38;5;241m.\u001B[39mescape(ent\u001B[38;5;241m.\u001B[39mtext), fake\u001B[38;5;241m.\u001B[39mcountry(), text)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m ent\u001B[38;5;241m.\u001B[39mlabel_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCARDINAL\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 49\u001B[0m     text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(re\u001B[38;5;241m.\u001B[39mescape(ent\u001B[38;5;241m.\u001B[39mtext), \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mclose_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43ment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m, text)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m ent\u001B[38;5;241m.\u001B[39mlabel_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mORDINAL\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     51\u001B[0m     text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(re\u001B[38;5;241m.\u001B[39mescape(ent\u001B[38;5;241m.\u001B[39mtext), fake_ordinal(), text)\n",
      "Cell \u001B[1;32mIn[18], line 7\u001B[0m, in \u001B[0;36mclose_number\u001B[1;34m(original_number)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclose_number\u001B[39m(original_number):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Convert the input to a float and then round it to the nearest integer\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m     num \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m(\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moriginal_number\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Generate a number within ±10% of the original number or ±3, whichever is greater\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     percentage_variation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(num \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.1\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: '#'"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import re\n",
    "import random\n",
    "\n",
    "def close_number(original_number):\n",
    "    try:\n",
    "        num = int(original_number)\n",
    "        # Generate a number within ±10% of the original number or ±3, whichever is greater\n",
    "        percentage_variation = int(num * 0.1)\n",
    "        min_variation = 3  # Minimum variation\n",
    "        variation = max(min_variation, percentage_variation)\n",
    "        return str(random.randint(max(0, num - variation), num + variation))\n",
    "    except ValueError:\n",
    "        # Return the original number if it's not an integer\n",
    "        return original_number\n",
    "    \n",
    "def fake_ordinal():\n",
    "    number = fake.random_int(min=1, max=100)\n",
    "    suffix = [\"th\", \"st\", \"nd\", \"rd\"] + [\"th\"] * 6\n",
    "    return str(number) + suffix[number % 10 if number % 100 not in [11, 12, 13] else 0]\n",
    "    \n",
    "fake = Faker()\n",
    "def replace_pii_with_fake(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Replace Twitter @username with @ followed by a fake first name\n",
    "    text = re.sub(r'@(\\w+)', lambda x: '@' + fake.first_name(), text)\n",
    "\n",
    "    # Iterate over the identified entities\n",
    "    for ent in doc.ents:\n",
    "        # Replace with fake data based on the entity type\n",
    "        if ent.label_ == 'PERSON':\n",
    "            # Ensure we're not replacing Twitter usernames\n",
    "            if not ent.text.startswith('@'):\n",
    "                text = re.sub(re.escape(ent.text), fake.name(), text)\n",
    "        elif ent.label_ == 'GPE':\n",
    "            text = re.sub(re.escape(ent.text), fake.city(), text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            text = re.sub(re.escape(ent.text), fake.date(), text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            text = re.sub(re.escape(ent.text), fake.company(), text)\n",
    "        elif ent.label_ == 'NORP':\n",
    "            text = re.sub(re.escape(ent.text), fake.country(), text)\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            text = re.sub(re.escape(ent.text), lambda x: close_number(ent.text), text)\n",
    "        elif ent.label_ == 'ORDINAL':\n",
    "            text = re.sub(re.escape(ent.text), fake_ordinal(), text)\n",
    "        elif ent.label_ == 'TIME':\n",
    "            text = re.sub(re.escape(ent.text), fake.time(), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(replace_pii_with_fake)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T20:04:07.275470600Z",
     "start_time": "2024-01-27T20:04:06.576986600Z"
    }
   },
   "id": "3e67063453962912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, that we anonymized seven of the 18 total categories, lets continue with the other 11, that are still missing: \n",
    "- ORDINAL PRODUCT MONEY WORK_OF_ART LOC FAC QUANTITY EVENT PERCENT LANGUAGE LAW\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f04675cea0f130f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T20:04:07.270547500Z"
    }
   },
   "id": "3517b896a417891d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Analyse the text to determine if any information can be obtained after the transformation process. What conclusions can you draw from this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d24d03b52d742b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idea is to check the semantics of text from the original dataset, the anonymized dataset and see if they are similar. Then, check if the PII from the original dataset are still present in the anonymized dataset\n",
    "\n",
    "Cosine Similarity: Cosine similarity measures the cosine of the angle between two non-zero vectors in a multidimensional space. Its value ranges from -1 to 1, where:\n",
    "\n",
    "1 means the vectors are identical.\n",
    "0 indicates orthogonality (no similarity).\n",
    "-1 implies completely opposite.\n",
    "\n",
    "By using 1 - cosine, we transform the scale:\n",
    "If the cosine similarity is 1 (vectors are identical), 1 - 1 becomes 0, indicating no difference.\n",
    "If the cosine similarity is 0 (vectors are orthogonal), 1 - 0 becomes 1, indicating maximum difference.\n",
    "A cosine similarity of -1 (completely opposite vectors) would result in a transformed similarity of 2, which typically doesn't occur in normalized vector spaces used in text analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece9a46f291c534d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")  # Make sure you've loaded the original dataset into 'df'\n",
    "df_anonymized = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Modify the get_embedding function to send inputs to the GPU\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2, tokenizer, model):\n",
    "    emb1 = get_embedding(text1, tokenizer, model)\n",
    "    emb2 = get_embedding(text2, tokenizer, model)\n",
    "    # Ensure embeddings are 1-D\n",
    "    emb1 = np.squeeze(emb1)\n",
    "    emb2 = np.squeeze(emb2)\n",
    "    #print(text1 + \" and \" + text2)\n",
    "    return 1 - cosine(emb1, emb2)\n",
    "\n",
    "# Calculate similarities\n",
    "try:\n",
    "    similarity_scores = [semantic_similarity(orig, anon, tokenizer, model) for orig, anon in zip(df['content'], df_anonymized['content'])]\n",
    "except ValueError as e:\n",
    "    print(f\"Error calculating similarity: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-27T20:04:07.272884300Z"
    }
   },
   "id": "f016ef0d4f4587e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
