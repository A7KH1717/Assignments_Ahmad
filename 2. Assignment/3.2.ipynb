{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Anonymizing your dataset\n",
    "- Goals: The goal of k-anonymity is to modify a dataset such that any given record cannot be distinguished from at least k−1 other records regarding certain \"quasi-identifier\" attributes.\n",
    "- Our identified quasi-identifiers from 3.1: 'region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b781a2e32b01fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting point: get a k-anonymized data set\n",
    "- To k-anonymize our given data set, we are using the Mondrian Multidimensional K-Anonymity approach\n",
    "- similar to other k-anonymity approaches, a simple and efficient greedy approximation algorithm is implemented to reduce complexity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dfbbc60eed6e1e1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   athlete_id           name               region          team  \\\n0        2554      Pj Ablang           South West   Double Edge   \n1        3517  Derek Abdella                                NaN   \n2        4691            NaN                                NaN   \n3        5164    Abo Brandon  Southern California  LAX CrossFit   \n4        5286    Bryce Abbey                                NaN   \n\n              affiliate gender   age  height  weight   fran  ...  deadlift  \\\n0  Double Edge CrossFit   Male  24.0    70.0   166.0    NaN  ...     400.0   \n1                   NaN   Male  42.0    70.0   190.0    NaN  ...       NaN   \n2                   NaN          0.0     0.0     0.0    NaN  ...       NaN   \n3          LAX CrossFit   Male  40.0    67.0     0.0  211.0  ...     375.0   \n4                   NaN   Male  32.0    65.0   149.0  206.0  ...       NaN   \n\n   backsq  pullups                                                eat  \\\n0   305.0      NaN                                                NaN   \n1     NaN      NaN                                                NaN   \n2     NaN      NaN                                                NaN   \n3   325.0     25.0               I eat 1-3 full cheat meals per week|   \n4   325.0     50.0  I eat quality foods but don't measure the amount|   \n\n                                               train  \\\n0  I workout mostly at a CrossFit Affiliate|I hav...   \n1  I have a coach who determines my programming|I...   \n2                                                NaN   \n3  I workout mostly at a CrossFit Affiliate|I hav...   \n4  I workout mostly at a CrossFit Affiliate|I inc...   \n\n                                          background  \\\n0  I played youth or high school level sports|I r...   \n1        I played youth or high school level sports|   \n2                                                NaN   \n3        I played youth or high school level sports|   \n4                           I played college sports|   \n\n                                          experience  \\\n0  I began CrossFit with a coach (e.g. at an affi...   \n1  I began CrossFit with a coach (e.g. at an affi...   \n2                                                NaN   \n3  I began CrossFit by trying it alone (without a...   \n4  I began CrossFit by trying it alone (without a...   \n\n                                            schedule     howlong  \\\n0         I do multiple workouts in a day 2x a week|   4+ years|   \n1         I do multiple workouts in a day 2x a week|   4+ years|   \n2                                                NaN         NaN   \n3                 I usually only do 1 workout a day|   4+ years|   \n4  I usually only do 1 workout a day|I strictly s...  1-2 years|   \n\n   retrieved_datetime  \n0                 NaN  \n1                 NaN  \n2                 NaN  \n3                 NaN  \n4                 NaN  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>athlete_id</th>\n      <th>name</th>\n      <th>region</th>\n      <th>team</th>\n      <th>affiliate</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>fran</th>\n      <th>...</th>\n      <th>deadlift</th>\n      <th>backsq</th>\n      <th>pullups</th>\n      <th>eat</th>\n      <th>train</th>\n      <th>background</th>\n      <th>experience</th>\n      <th>schedule</th>\n      <th>howlong</th>\n      <th>retrieved_datetime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2554</td>\n      <td>Pj Ablang</td>\n      <td>South West</td>\n      <td>Double Edge</td>\n      <td>Double Edge CrossFit</td>\n      <td>Male</td>\n      <td>24.0</td>\n      <td>70.0</td>\n      <td>166.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>400.0</td>\n      <td>305.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I workout mostly at a CrossFit Affiliate|I hav...</td>\n      <td>I played youth or high school level sports|I r...</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I do multiple workouts in a day 2x a week|</td>\n      <td>4+ years|</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3517</td>\n      <td>Derek Abdella</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>42.0</td>\n      <td>70.0</td>\n      <td>190.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I have a coach who determines my programming|I...</td>\n      <td>I played youth or high school level sports|</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I do multiple workouts in a day 2x a week|</td>\n      <td>4+ years|</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4691</td>\n      <td>NaN</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5164</td>\n      <td>Abo Brandon</td>\n      <td>Southern California</td>\n      <td>LAX CrossFit</td>\n      <td>LAX CrossFit</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>67.0</td>\n      <td>0.0</td>\n      <td>211.0</td>\n      <td>...</td>\n      <td>375.0</td>\n      <td>325.0</td>\n      <td>25.0</td>\n      <td>I eat 1-3 full cheat meals per week|</td>\n      <td>I workout mostly at a CrossFit Affiliate|I hav...</td>\n      <td>I played youth or high school level sports|</td>\n      <td>I began CrossFit by trying it alone (without a...</td>\n      <td>I usually only do 1 workout a day|</td>\n      <td>4+ years|</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5286</td>\n      <td>Bryce Abbey</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>32.0</td>\n      <td>65.0</td>\n      <td>149.0</td>\n      <td>206.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>325.0</td>\n      <td>50.0</td>\n      <td>I eat quality foods but don't measure the amount|</td>\n      <td>I workout mostly at a CrossFit Affiliate|I inc...</td>\n      <td>I played college sports|</td>\n      <td>I began CrossFit by trying it alone (without a...</td>\n      <td>I usually only do 1 workout a day|I strictly s...</td>\n      <td>1-2 years|</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some adjustments on the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"kleine_athletes.csv\", index_col=False, low_memory=False)\n",
    "\n",
    "df['age'].fillna(0, inplace=True)\n",
    "df['height'].fillna(0, inplace=True)\n",
    "df['weight'].fillna(0, inplace=True)\n",
    "\n",
    "df['region'].fillna('', inplace=True)\n",
    "df['gender'].fillna('', inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.127648300Z",
     "start_time": "2024-01-12T09:49:36.076078600Z"
    }
   },
   "id": "6f02b8c8ecab030e",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "#Calculates and returns the spans (range of values) for each column in a specified partition of a dataframe, with an option to scale these spans by provided values.\n",
    "def get_spans(df, partition, scale=None):\n",
    "    spans = {}\n",
    "    for feature_column in quasi_identifiers:\n",
    "        if feature_column in categorical:\n",
    "            span = len(df[feature_column][partition].unique())\n",
    "        else:\n",
    "            span = df[feature_column][partition].max() - df[feature_column][partition].min()\n",
    "        if scale is not None:\n",
    "            span = span / scale[feature_column]\n",
    "        spans[feature_column] = span\n",
    "    return spans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.145178600Z",
     "start_time": "2024-01-12T09:49:36.131179Z"
    }
   },
   "id": "6c70412e40d9d7fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Divides a specified partition of a dataframe into two parts based on the median or unique values of a given column, returning a tuple with the indices of these two parts.\n",
    "def split(df, partition, column):\n",
    "    dfp = df[column][partition]\n",
    "    if column in categorical:\n",
    "        values = dfp.unique()\n",
    "        lv = set(values[:len(values) // 2])\n",
    "        rv = set(values[len(values) // 2:])\n",
    "        return dfp.index[dfp.isin(lv)], dfp.index[dfp.isin(rv)]\n",
    "    else:\n",
    "        median = dfp.median()\n",
    "        dfl = dfp.index[dfp < median]\n",
    "        dfr = dfp.index[dfp >= median]\n",
    "        return (dfl, dfr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.145178600Z",
     "start_time": "2024-01-12T09:49:36.136368400Z"
    }
   },
   "id": "91f3144e071e27cb",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Checks if a partition is k-anonymous by comparing its amount of entries with the required (k).\n",
    "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
    "    if len(partition) < k:\n",
    "        return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.172877700Z",
     "start_time": "2024-01-12T09:49:36.148225300Z"
    }
   },
   "id": "b9a2b33d03f26ba2",
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Partitions a dataframe into valid subsets based on specified feature columns, a sensitive column, and span scales, using a validity function to ensure each partition meets certain criteria.\n",
    "def partition_dataset(df, feature_columns, sensitive_column, scale, is_valid):\n",
    "    finished_partitions_temp = []\n",
    "    partitions = [df.index]\n",
    "    while partitions:\n",
    "        partition = partitions.pop(0)\n",
    "        spans = get_spans(df[feature_columns], partition, scale)\n",
    "        for column, span in sorted(spans.items(), key=lambda x: -x[1]):\n",
    "            lp, rp = split(df, partition, column)\n",
    "            if not is_valid(df, lp, sensitive_column) or not is_valid(df, rp, sensitive_column):\n",
    "                continue\n",
    "            partitions.extend((lp, rp))\n",
    "            break\n",
    "        else:\n",
    "            finished_partitions_temp.append(partition)\n",
    "    return finished_partitions_temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.190915600Z",
     "start_time": "2024-01-12T09:49:36.152042700Z"
    }
   },
   "id": "45ea8668dea21938",
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Aggregates the values of a series with categorical values by concatenating them.\n",
    "def agg_categorical_column(series):\n",
    "    # Check if the series is empty or if mode() returns an empty series\n",
    "    if series.empty or series.mode().empty:\n",
    "        return None  # or some default value or placeholder\n",
    "    else:\n",
    "        return series.mode().iloc[0]  # Safely access the first element of mode\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.190915600Z",
     "start_time": "2024-01-12T09:49:36.158663600Z"
    }
   },
   "id": "d260d0e832101ffd",
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def agg_numerical_column(series):\n",
    "    #return series.mean()\n",
    "    #print(series)\n",
    "    return series.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.223202600Z",
     "start_time": "2024-01-12T09:49:36.176083300Z"
    }
   },
   "id": "1f48f0e16c8b2940",
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Constructs an anonymized dataset by aggregating feature columns and sensitive columns separately for each partition.\n",
    "def build_anonymized_dataset(df, partitions, feature_columns, sensitive_columns,\n",
    "                                                  max_partitions=None):\n",
    "    aggregations = {}\n",
    "    for column in feature_columns:\n",
    "        if column in categorical:\n",
    "            aggregations[column] = agg_categorical_column\n",
    "        else:\n",
    "            aggregations[column] = agg_numerical_column\n",
    "    rows = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        if i % 100 == 1:\n",
    "            print(\"Finished {} partitions...\".format(i))\n",
    "        if max_partitions is not None and i > max_partitions:\n",
    "            break\n",
    "        grouped_columns = df.loc[partition].agg(aggregations, squeeze=False)\n",
    "        values = grouped_columns.to_dict()\n",
    "        # Iterate through each sensitive column and aggregate counts\n",
    "        for sensitive_column in sensitive_columns:\n",
    "            sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({sensitive_column: 'count'})\n",
    "            for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
    "                if count == 0:\n",
    "                    continue\n",
    "                sensitive_values = values.copy()\n",
    "                sensitive_values.update({\n",
    "                    sensitive_column: sensitive_value,\n",
    "                    'count': count,\n",
    "                })\n",
    "                rows.append(sensitive_values)\n",
    "    return pd.DataFrame(rows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.280343400Z",
     "start_time": "2024-01-12T09:49:36.198379Z"
    }
   },
   "id": "c4cf30a6fb09d064",
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#categorical attributes that need to be treated in another way than numerical\n",
    "categorical = {'schedule', 'howlong', 'region', 'gender', 'eat'}\n",
    "for name in categorical:\n",
    "    df[name] = df[name].astype('category')\n",
    "    \n",
    "#Our quasi identifiers that we identified earlier\n",
    "quasi_identifiers = ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong']\n",
    "\n",
    "# sensitive-values that should be taken into account\n",
    "sensitive_columns = ['athlete_id', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift',\n",
    "                     'backsq', 'pullups']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.294127600Z",
     "start_time": "2024-01-12T09:49:36.282345500Z"
    }
   },
   "id": "e92b677b7074e723",
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_spans = get_spans(df, df.index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:36.351939500Z",
     "start_time": "2024-01-12T09:49:36.296129Z"
    }
   },
   "id": "5190fc885bf63bf1",
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "78"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_partitions = partition_dataset(df, quasi_identifiers, sensitive_columns, full_spans, is_k_anonymous)\n",
    "len(finished_partitions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:37.319205100Z",
     "start_time": "2024-01-12T09:49:36.304034700Z"
    }
   },
   "id": "3904e62d591e3ec3",
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 partitions...\n"
     ]
    }
   ],
   "source": [
    "dfn = build_anonymized_dataset(df, finished_partitions, quasi_identifiers, sensitive_columns)\n",
    "# we sort the resulting dataframe using the feature columns and the sensitive attribute\n",
    "#dfn.sort_values()\n",
    "\n",
    "dfn.to_csv(\"k3_anon_klein.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:38.656196600Z",
     "start_time": "2024-01-12T09:49:37.315074100Z"
    }
   },
   "id": "9b0bd1ba37772d37",
   "execution_count": 136
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3adee71c0da5d1d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extending the k-anonymized dataset to also be l-diverse\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfd5955484476ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def diversity(df, partition, column):\n",
    "    print(f\"Partition indices: {partition}\")\n",
    "    \n",
    "    data = df.loc[partition, column].squeeze()\n",
    "    #print(f\"Partition: {partition}\")\n",
    "    #print(f\"Data: {data}\")\n",
    "    unique_values = data.unique() if isinstance(data, pd.Series) else [data]\n",
    "    #print(f\"Unique values: {unique_values}\")\n",
    "    return len(unique_values)\n",
    "\n",
    "def is_l_diverse(df, partition, sensitive_column, l=2):\n",
    "    return diversity(df, partition, sensitive_column) >= l"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:38.666261500Z",
     "start_time": "2024-01-12T09:49:38.656196600Z"
    }
   },
   "id": "8214935225e196ef",
   "execution_count": 137
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition indices: Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "       ...\n",
      "       488, 489, 490, 491, 493, 494, 495, 496, 497, 498],\n",
      "      dtype='int64', length=469)\n",
      "Partition indices: Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "       ...\n",
      "       489, 490, 491, 492, 493, 494, 495, 496, 497, 498],\n",
      "      dtype='int64', length=475)\n",
      "Partition indices: Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "       ...\n",
      "       489, 490, 491, 492, 493, 494, 495, 496, 497, 498],\n",
      "      dtype='int64', length=476)\n",
      "Partition indices: Index([  2,   3,   7,  11,  14,  19,  24,  26,  28,  29,\n",
      "       ...\n",
      "       480, 482, 483, 485, 486, 488, 493, 494, 497, 498],\n",
      "      dtype='int64', length=249)\n",
      "Partition indices: Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "       ...\n",
      "       487, 488, 489, 490, 492, 493, 494, 495, 497, 498],\n",
      "      dtype='int64', length=434)\n",
      "Partition indices: Index([  0,   2,   6,   7,  11,  15,  17,  19,  24,  29,\n",
      "       ...\n",
      "       479, 480, 481, 482, 483, 486, 488, 489, 497, 498],\n",
      "      dtype='int64', length=244)\n",
      "Partition indices: Index([  0,   1,   3,   4,   5,   6,   8,   9,  10,  12,\n",
      "       ...\n",
      "       485, 487, 489, 490, 491, 492, 493, 494, 495, 496],\n",
      "      dtype='int64', length=317)\n"
     ]
    }
   ],
   "source": [
    "df_ldiverse = pd.read_csv(\"k3_anon_klein.csv\", index_col=False, low_memory=False)\n",
    "full_spans = get_spans(df_ldiverse, df_ldiverse.index)\n",
    "\n",
    "finished_l_diverse_partitions = partition_dataset(\n",
    "    df, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))\n",
    "\n",
    "dfldiverse_finished = build_anonymized_dataset(df_ldiverse, finished_l_diverse_partitions, quasi_identifiers, sensitive_columns)\n",
    "dfldiverse_finished.to_csv(\"l2_diverse.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:38.710553300Z",
     "start_time": "2024-01-12T09:49:38.660623700Z"
    }
   },
   "id": "32da8f97ac74169c",
   "execution_count": 138
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extending the k-anonymized dataset to also achieve t-closeness"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32954f7c4f93f649"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e927c25a3c6c2d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def t_closeness(df, partition, sensitive_columns, global_freqs, t=0.3, categorical=None):\n",
    "    # Helper function for numerical t-closeness\n",
    "    def t_closeness_numerical(full_data, partition_data):\n",
    "        ks_stat, _ = ks_2samp(full_data, partition_data)\n",
    "        return ks_stat\n",
    "\n",
    "    # Helper function for categorical t-closeness\n",
    "    def t_closeness_categorical(partition_data, global_freqs):\n",
    "        total_count = float(len(partition_data))\n",
    "        d_max = None\n",
    "        group_counts = partition_data.groupby(column)[column].agg('count')\n",
    "        for value, count in group_counts.to_dict().items():\n",
    "            p = count / total_count\n",
    "            d = abs(p - global_freqs[value])\n",
    "            if d_max is None or d > d_max:\n",
    "                d_max = d\n",
    "        return d_max\n",
    "\n",
    "    # Main t-closeness logic\n",
    "    for column in sensitive_columns:\n",
    "        full_data = df[column]\n",
    "        partition_data = df.loc[partition, column]\n",
    "        \n",
    "        if categorical and column in categorical:\n",
    "            distance = t_closeness_categorical(partition_data, global_freqs[column])\n",
    "        else:\n",
    "            distance = t_closeness_numerical(full_data, partition_data)\n",
    "\n",
    "        if distance > t:\n",
    "            return False\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:39.049470300Z",
     "start_time": "2024-01-12T09:49:38.706543200Z"
    }
   },
   "id": "c03f0e8542d8b261",
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133],\n",
      "      dtype='int64', length=1654)\n",
      "Partition indices: Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "       ...\n",
      "       172, 173, 174, 175, 176, 177, 178, 179, 180, 181],\n",
      "      dtype='int64', length=182)\n",
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112],\n",
      "      dtype='int64', length=1054)\n",
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133],\n",
      "      dtype='int64', length=1046)\n",
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133],\n",
      "      dtype='int64', length=1052)\n",
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133],\n",
      "      dtype='int64', length=1602)\n",
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133],\n",
      "      dtype='int64', length=1517)\n",
      "Partition indices: Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "       ...\n",
      "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133],\n",
      "      dtype='int64', length=943)\n"
     ]
    }
   ],
   "source": [
    "df_tclose = pd.read_csv(\"k3_anon_klein.csv\", index_col=False, low_memory=False)\n",
    "full_spans = get_spans(df_tclose, df_tclose.index)\n",
    "\n",
    "# Get the global frequencies for the sensitive column\n",
    "global_frequencies = {sensitive_column: {} for sensitive_column in sensitive_columns}\n",
    "total_count = len(df)\n",
    "\n",
    "# Determine frequency for every sensitive attribute\n",
    "for sensitive_column in sensitive_columns:\n",
    "    group_counts = df_tclose.groupby(sensitive_column, observed=False)[sensitive_column].agg('count')\n",
    "    for value, count in group_counts.to_dict().items():\n",
    "        p = count / total_count\n",
    "        global_frequencies[sensitive_column][value] = p\n",
    "\n",
    "finished_l_closepartitions = partition_dataset(\n",
    "    df_tclose, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))\n",
    "\n",
    "dfclose_finished = build_anonymized_dataset(df_tclose, finished_l_closepartitions, quasi_identifiers, sensitive_columns)\n",
    "dfclose_finished.to_csv(\"t_close.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T09:49:39.114414100Z",
     "start_time": "2024-01-12T09:49:39.052513600Z"
    }
   },
   "id": "2790869de5d00a83",
   "execution_count": 140
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
