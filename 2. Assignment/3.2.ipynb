{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Anonymizing your dataset - 20 marks\n",
    "- Goals: The goal of k-anonymity is to modify a dataset such that any given record cannot be distinguished from at least kâˆ’1 other records regarding certain \"quasi-identifier\" attributes.\n",
    "- Our identified quasi-identifiers from 3.1: 'region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b781a2e32b01fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting point: get a k-anonymized data set as a base\n",
    "- To k-anonymize our given data set, we are using the Mondrian Multidimensional K-Anonymity approach to build on later for t-closeness and l-diversity.\n",
    "- similar to other k-anonymity approaches, a simple and efficient greedy approximation algorithm is implemented to reduce complexity.\n",
    "\n",
    "Because of runtime issues (the athletes.csv runs for more than a few hours with this jupyter notebook) we decided to take the first 20k entries of the athletes dataset to showcase the algorithm. \n",
    "We start by implementing a few functions which will then later be used to k=3 anonymize our dataset: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dfbbc60eed6e1e1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423006\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"athletes.csv\", index_col=False, low_memory=False)\n",
    "print(df.shape[0])\n",
    "\n",
    "# Select the first 10,000 rows\n",
    "df_reduced = df.iloc[:20000, :]\n",
    "\n",
    "# Save the reduced dataset to a new CSV file\n",
    "df_reduced.to_csv(\"reduced_athletes.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.229200200Z",
     "start_time": "2024-01-14T15:24:24.240997300Z"
    }
   },
   "id": "60a056d9854164ad",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Some adjustments on the dataset\n",
    "df = pd.read_csv(\"reduced_athletes.csv\", index_col=False, low_memory=False)\n",
    "\n",
    "df['age'].fillna(0, inplace=True)\n",
    "df['height'].fillna(0, inplace=True)\n",
    "df['weight'].fillna(0, inplace=True)\n",
    "\n",
    "df['region'].fillna('', inplace=True)\n",
    "df['gender'].fillna('', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.300177100Z",
     "start_time": "2024-01-14T15:24:26.222674500Z"
    }
   },
   "id": "6f02b8c8ecab030e",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "#Calculates and returns the spans (range of values) for each column in a specified partition of a dataframe, with an option to scale these spans by provided values.\n",
    "def get_spans(df, partition, scale=None):\n",
    "    spans = {}\n",
    "    for feature_column in quasi_identifiers:\n",
    "        if feature_column in categorical:\n",
    "            span = len(df[feature_column][partition].unique())\n",
    "        else:\n",
    "            span = df[feature_column][partition].max() - df[feature_column][partition].min()\n",
    "        if scale is not None:\n",
    "            span = span / scale[feature_column]\n",
    "        spans[feature_column] = span\n",
    "    return spans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.312227100Z",
     "start_time": "2024-01-14T15:24:26.302886500Z"
    }
   },
   "id": "6c70412e40d9d7fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Divides a specified partition of a dataframe into two parts based on the median or unique values of a given column, returning a tuple with the indices of these two parts.\n",
    "def split(df, partition, column):\n",
    "    dfp = df[column][partition]\n",
    "    if column in categorical:\n",
    "        values = dfp.unique()\n",
    "        lv = set(values[:len(values) // 2])\n",
    "        rv = set(values[len(values) // 2:])\n",
    "        return dfp.index[dfp.isin(lv)], dfp.index[dfp.isin(rv)]\n",
    "    else:\n",
    "        median = dfp.median()\n",
    "        dfl = dfp.index[dfp < median]\n",
    "        dfr = dfp.index[dfp >= median]\n",
    "        return (dfl, dfr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.313647100Z",
     "start_time": "2024-01-14T15:24:26.306904500Z"
    }
   },
   "id": "91f3144e071e27cb",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Checks if a partition is k-anonymous by comparing its amount of entries with the required (k).\n",
    "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
    "    if len(partition) < k:\n",
    "        return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.318224400Z",
     "start_time": "2024-01-14T15:24:26.312227100Z"
    }
   },
   "id": "b9a2b33d03f26ba2",
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Partitions a dataframe into valid subsets based on specified feature columns, a sensitive column, and span scales, using a validity function to ensure each partition meets certain criteria.\n",
    "def partition_dataset(df, feature_columns, sensitive_column, scale, is_valid):\n",
    "    finished_partitions_temp = []\n",
    "    partitions = [df.index]\n",
    "    while partitions:\n",
    "        partition = partitions.pop(0)\n",
    "        spans = get_spans(df[feature_columns], partition, scale)\n",
    "        for column, span in sorted(spans.items(), key=lambda x: -x[1]):\n",
    "            lp, rp = split(df, partition, column)\n",
    "            if not is_valid(df, lp, sensitive_column) or not is_valid(df, rp, sensitive_column):\n",
    "                continue\n",
    "            partitions.extend((lp, rp))\n",
    "            break\n",
    "        else:\n",
    "            finished_partitions_temp.append(partition)\n",
    "    return finished_partitions_temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.339819700Z",
     "start_time": "2024-01-14T15:24:26.319223500Z"
    }
   },
   "id": "45ea8668dea21938",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Aggregates the values of a series with categorical values by concatenating them.\n",
    "def agg_categorical_column(series):\n",
    "    # Check if the series is empty or if mode() returns an empty series\n",
    "    if series.empty or series.mode().empty:\n",
    "        return None  # or some default value or placeholder\n",
    "    else:\n",
    "        return series.mode().iloc[0]  # access the first element of mode\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.340893100Z",
     "start_time": "2024-01-14T15:24:26.324419500Z"
    }
   },
   "id": "d260d0e832101ffd",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def agg_numerical_column(series):\n",
    "    return series.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.342024700Z",
     "start_time": "2024-01-14T15:24:26.329685600Z"
    }
   },
   "id": "1f48f0e16c8b2940",
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Constructs an anonymized dataset by aggregating feature columns and sensitive columns separately for each partition.\n",
    "def build_anonymized_dataset(df, partitions, feature_columns, sensitive_columns,\n",
    "                                                  max_partitions=None):\n",
    "    aggregations = {}\n",
    "    for column in feature_columns:\n",
    "        if column in categorical:\n",
    "            aggregations[column] = agg_categorical_column\n",
    "        else:\n",
    "            aggregations[column] = agg_numerical_column\n",
    "    rows = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        if max_partitions is not None and i > max_partitions:\n",
    "            break\n",
    "        grouped_columns = df.loc[partition].agg(aggregations, squeeze=False)\n",
    "        values = grouped_columns.to_dict()\n",
    "        # Iterate through each sensitive column and aggregate counts\n",
    "        for sensitive_column in sensitive_columns:\n",
    "            sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({sensitive_column: 'count'})\n",
    "            for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
    "                if count == 0:\n",
    "                    continue\n",
    "                sensitive_values = values.copy()\n",
    "                sensitive_values.update({\n",
    "                    sensitive_column: sensitive_value,\n",
    "                    'count': count,\n",
    "                })\n",
    "                rows.append(sensitive_values)\n",
    "    return pd.DataFrame(rows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:24:26.342546600Z",
     "start_time": "2024-01-14T15:24:26.336717600Z"
    }
   },
   "id": "c4cf30a6fb09d064",
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting to k-anonymize with k=3\n",
    "1. First, we define our quasi-identifiers that we chose in 3.1. \n",
    "2. Secondly, we divide our quasi identifiers in two categories\n",
    "   - categorical: schedule, howlong, region, gender, eat\n",
    "      - These attributes need to be treated differently because they cant be compared like numerical attributes\n",
    "   - numerical: age, height, weight\n",
    "      - These attributes need no special treatment\n",
    "3. We then start with k-anonymizing our dataset by calculating the spans of our dataframe and passing the result on to partition our dataset\n",
    "4. the partitions then get aggregated with respect to our categorical and numerical attributes \n",
    "5. the finished dataframe then gets saved to a new .csv "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee717028bfc8d1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#categorical attributes that need to be treated in another way than numerical\n",
    "categorical = {'schedule', 'howlong', 'region', 'gender', 'eat'}\n",
    "for name in categorical:\n",
    "    df[name] = df[name].astype('category')\n",
    "    \n",
    "#Our quasi identifiers that we identified earlier\n",
    "quasi_identifiers = ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong']\n",
    "\n",
    "# sensitive-values that should be taken into account\n",
    "sensitive_columns = ['athlete_id', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift',\n",
    "                     'backsq', 'pullups']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:56:58.108623800Z",
     "start_time": "2024-01-14T15:56:58.102809200Z"
    }
   },
   "id": "e92b677b7074e723",
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910\n"
     ]
    }
   ],
   "source": [
    "full_spans = get_spans(df, df.index)\n",
    "finished_partitions = partition_dataset(df, quasi_identifiers, sensitive_columns, full_spans, is_k_anonymous)\n",
    "print(len(finished_partitions))\n",
    "\n",
    "dfn = build_anonymized_dataset(df, finished_partitions, quasi_identifiers, sensitive_columns)\n",
    "\n",
    "dfn.to_csv(\"k_anon.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:59:40.919912200Z",
     "start_time": "2024-01-14T15:58:42.795095200Z"
    }
   },
   "id": "9b0bd1ba37772d37",
   "execution_count": 131
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3adee71c0da5d1d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extending the k-anonymized dataset to also be l-diverse\n",
    "- l-diversity requires that for every group of records sharing  quasi-identifier attributes, there are at least 'l' \"well-represented\" values for the sensitive attribute.\n",
    "- l=2 diversity: Specifically, this means that in each group of records with the same quasi-identifiers, there should be at least two distinct values for the sensitive attributes. The idea is to prevent attackers from deducing the value of a sensitive attribute within a group. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfd5955484476ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Calculates the diversity of a partition of a dataframe on a given column.\n",
    "def diversity(df, partition, column):\n",
    "    return len(df[column][partition].unique())\n",
    "\n",
    "def is_l_diverse(df, partition, sensitive_columns, l=2):\n",
    "       for sensitive_column in sensitive_columns:\n",
    "        if diversity(df, partition, sensitive_column) < l:      #if partition is not l-diverse\n",
    "            return False\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:25:23.426452500Z",
     "start_time": "2024-01-14T15:25:23.417297800Z"
    }
   },
   "id": "8214935225e196ef",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2849\n"
     ]
    }
   ],
   "source": [
    "df_ldiverse = pd.read_csv(\"k_anon.csv\", index_col=False, low_memory=False)\n",
    "full_spans = get_spans(df_ldiverse, df_ldiverse.index)\n",
    "\n",
    "finished_l_diverse_partitions = partition_dataset(\n",
    "    df_ldiverse, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    is_l_diverse)\n",
    "print(len(finished_l_diverse_partitions))   #How many partitions are there\n",
    "\n",
    "dfldiverse_finished = build_anonymized_dataset(df_ldiverse, finished_l_diverse_partitions, quasi_identifiers, sensitive_columns)\n",
    "dfldiverse_finished.to_csv(\"l_diverse.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:26:28.364391900Z",
     "start_time": "2024-01-14T15:25:23.423429600Z"
    }
   },
   "id": "32da8f97ac74169c",
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extending the k-anonymized dataset to also achieve t-closeness\n",
    "- A dataset is said to have t-closeness if the distribution of a sensitive attribute in any given group is not more than 't' different from the distribution of the attribute in the overall dataset.\n",
    "- Unlike l-diversity, which focuses on the variety of sensitive attributes, t-closeness concerns itself with the distribution (frequency) of these attributes.\n",
    "- A smaller t value indicates a stricter requirement for maintaining the distribution of the dataset. (Here: t=0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32954f7c4f93f649"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e927c25a3c6c2d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# calculate t-closeness for numerical values \n",
    "def t_closeness_numerical(df, partition, column):\n",
    "    full_data = df[column]\n",
    "    partition_data = df.loc[partition, column]\n",
    "\n",
    "    # Check if either dataset is empty\n",
    "    if full_data.empty or partition_data.empty:\n",
    "        # Generate and return a random float between 30 and 200\n",
    "        random_float = random.uniform(30, 200)\n",
    "        return random_float\n",
    "\n",
    "    ks_stat, _ = ks_2samp(full_data, partition_data)\n",
    "    return ks_stat\n",
    "\n",
    "# calculate t-closeness for categorical values\n",
    "def t_closeness_categorical(df, partition, column, global_freqs):\n",
    "    total_count = float(len(partition))\n",
    "    d_max = None\n",
    "    group_counts = df.loc[partition].groupby(column, observed=False)[column].agg('count')\n",
    "    for value, count in group_counts.to_dict().items():\n",
    "        p = count / total_count\n",
    "        d = abs(p - global_freqs[value])\n",
    "        if d_max is None or d > d_max:\n",
    "            d_max = d\n",
    "    return d_max\n",
    "\n",
    "# check if partition is t-close\n",
    "def is_t_close(df, partition, sensitive_columns, global_freqs, t=0.2):\n",
    "    for sensitive_column in sensitive_columns:\n",
    "        if sensitive_column not in categorical:\n",
    "            distance = t_closeness_numerical(df, partition, sensitive_column)\n",
    "        else:\n",
    "            distance = t_closeness_categorical(df, partition, sensitive_column, global_freqs[sensitive_column])\n",
    "        if distance > t:\n",
    "            return False\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:53:08.031280700Z",
     "start_time": "2024-01-14T15:53:08.011640400Z"
    }
   },
   "id": "c03f0e8542d8b261",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950\n"
     ]
    }
   ],
   "source": [
    "df_tclose = pd.read_csv(\"k_anon.csv\", index_col=False, low_memory=False)\n",
    "full_spans = get_spans(df_tclose, df_tclose.index)\n",
    "\n",
    "# Get the global frequencies for the sensitive column\n",
    "global_frequencies = {sensitive_column: {} for sensitive_column in sensitive_columns}\n",
    "total_count = len(df)\n",
    "\n",
    "# Determine frequency for every sensitive attribute\n",
    "for sensitive_column in sensitive_columns:\n",
    "    group_counts = df_tclose.groupby(sensitive_column, observed=False)[sensitive_column].agg('count')\n",
    "    for value, count in group_counts.to_dict().items():\n",
    "        p = count / total_count\n",
    "        global_frequencies[sensitive_column][value] = p\n",
    "\n",
    "finished_t_closepartitions = partition_dataset(\n",
    "    df_tclose, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    lambda *args: is_t_close(*args, global_frequencies))\n",
    "print(len(finished_t_closepartitions))\n",
    "\n",
    "\n",
    "dfclose_finished = build_anonymized_dataset(df_tclose, finished_t_closepartitions, quasi_identifiers, sensitive_columns)\n",
    "dfclose_finished.to_csv(\"t_close.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:56:08.917712800Z",
     "start_time": "2024-01-14T15:53:08.717339300Z"
    }
   },
   "id": "2790869de5d00a83",
   "execution_count": 127
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discussing the results\n",
    "Our datasets are characterized by three privacy-preserving measures: \n",
    "k=3 anonymity, l=2 diversity, and t=0.2 closeness. These measures are used to ensure that the data can be used for analysis without compromising the privacy of the individuals represented in the data. \n",
    "- k = 3 ensures that any individual's data cannot be distinguished from at least two other individuals within the dataset. Protects against identification risks.\n",
    "- l = 2 diversity: ensures variety in the sensitive attribute in each group of the dataset. In a dataset with l=2 diversity, each group of records must have at least two different values for the sensitive attribute. Protects against attribute disclosure.\n",
    "- t = 0.2  ensures the preservation of data utility by maintaining a consistent distribution of sensitive attributes.\n",
    "By combining these three techniques, we can achieve decent privacy protection of our given dataset. Each measure addresses a different aspect of privacy.\n",
    "\n",
    "The rest of the comparison / discussion of results happen in 3.4. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4fb78b577dcd6ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
