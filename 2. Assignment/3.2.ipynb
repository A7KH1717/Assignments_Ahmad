{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Anonymizing your dataset\n",
    "- Goals: The goal of k-anonymity is to modify a dataset such that any given record cannot be distinguished from at least k−1 other records regarding certain \"quasi-identifier\" attributes.\n",
    "- Our identified quasi-identifiers from 3.1: 'region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b781a2e32b01fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting point: get a k-anonymized data set\n",
    "- To k-anonymize our given data set, we are using the Mondrian Multidimensional K-Anonymity approach\n",
    "- similar to other k-anonymity approaches, a simple and efficient greedy approximation algorithm is implemented to reduce complexity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dfbbc60eed6e1e1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   athlete_id           name               region          team  \\\n0      2554.0      Pj Ablang           South West   Double Edge   \n1      3517.0  Derek Abdella                                NaN   \n2      4691.0            NaN                                NaN   \n3      5164.0    Abo Brandon  Southern California  LAX CrossFit   \n4      5286.0    Bryce Abbey                                NaN   \n\n              affiliate gender   age  height  weight   fran  ...  deadlift  \\\n0  Double Edge CrossFit   Male  24.0    70.0   166.0    NaN  ...     400.0   \n1                   NaN   Male  42.0    70.0   190.0    NaN  ...       NaN   \n2                   NaN          0.0     0.0     0.0    NaN  ...       NaN   \n3          LAX CrossFit   Male  40.0    67.0     0.0  211.0  ...     375.0   \n4                   NaN   Male  32.0    65.0   149.0  206.0  ...       NaN   \n\n   backsq  pullups                                                eat  \\\n0   305.0      NaN                                                NaN   \n1     NaN      NaN                                                NaN   \n2     NaN      NaN                                                NaN   \n3   325.0     25.0               I eat 1-3 full cheat meals per week|   \n4   325.0     50.0  I eat quality foods but don't measure the amount|   \n\n                                               train  \\\n0  I workout mostly at a CrossFit Affiliate|I hav...   \n1  I have a coach who determines my programming|I...   \n2                                                NaN   \n3  I workout mostly at a CrossFit Affiliate|I hav...   \n4  I workout mostly at a CrossFit Affiliate|I inc...   \n\n                                          background  \\\n0  I played youth or high school level sports|I r...   \n1        I played youth or high school level sports|   \n2                                                NaN   \n3        I played youth or high school level sports|   \n4                           I played college sports|   \n\n                                          experience  \\\n0  I began CrossFit with a coach (e.g. at an affi...   \n1  I began CrossFit with a coach (e.g. at an affi...   \n2                                                NaN   \n3  I began CrossFit by trying it alone (without a...   \n4  I began CrossFit by trying it alone (without a...   \n\n                                            schedule     howlong  \\\n0         I do multiple workouts in a day 2x a week|   4+ years|   \n1         I do multiple workouts in a day 2x a week|   4+ years|   \n2                                                NaN         NaN   \n3                 I usually only do 1 workout a day|   4+ years|   \n4  I usually only do 1 workout a day|I strictly s...  1-2 years|   \n\n   retrieved_datetime  \n0                 NaN  \n1                 NaN  \n2                 NaN  \n3                 NaN  \n4                 NaN  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>athlete_id</th>\n      <th>name</th>\n      <th>region</th>\n      <th>team</th>\n      <th>affiliate</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>fran</th>\n      <th>...</th>\n      <th>deadlift</th>\n      <th>backsq</th>\n      <th>pullups</th>\n      <th>eat</th>\n      <th>train</th>\n      <th>background</th>\n      <th>experience</th>\n      <th>schedule</th>\n      <th>howlong</th>\n      <th>retrieved_datetime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2554.0</td>\n      <td>Pj Ablang</td>\n      <td>South West</td>\n      <td>Double Edge</td>\n      <td>Double Edge CrossFit</td>\n      <td>Male</td>\n      <td>24.0</td>\n      <td>70.0</td>\n      <td>166.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>400.0</td>\n      <td>305.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I workout mostly at a CrossFit Affiliate|I hav...</td>\n      <td>I played youth or high school level sports|I r...</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I do multiple workouts in a day 2x a week|</td>\n      <td>4+ years|</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3517.0</td>\n      <td>Derek Abdella</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>42.0</td>\n      <td>70.0</td>\n      <td>190.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I have a coach who determines my programming|I...</td>\n      <td>I played youth or high school level sports|</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I do multiple workouts in a day 2x a week|</td>\n      <td>4+ years|</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4691.0</td>\n      <td>NaN</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5164.0</td>\n      <td>Abo Brandon</td>\n      <td>Southern California</td>\n      <td>LAX CrossFit</td>\n      <td>LAX CrossFit</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>67.0</td>\n      <td>0.0</td>\n      <td>211.0</td>\n      <td>...</td>\n      <td>375.0</td>\n      <td>325.0</td>\n      <td>25.0</td>\n      <td>I eat 1-3 full cheat meals per week|</td>\n      <td>I workout mostly at a CrossFit Affiliate|I hav...</td>\n      <td>I played youth or high school level sports|</td>\n      <td>I began CrossFit by trying it alone (without a...</td>\n      <td>I usually only do 1 workout a day|</td>\n      <td>4+ years|</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5286.0</td>\n      <td>Bryce Abbey</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>32.0</td>\n      <td>65.0</td>\n      <td>149.0</td>\n      <td>206.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>325.0</td>\n      <td>50.0</td>\n      <td>I eat quality foods but don't measure the amount|</td>\n      <td>I workout mostly at a CrossFit Affiliate|I inc...</td>\n      <td>I played college sports|</td>\n      <td>I began CrossFit by trying it alone (without a...</td>\n      <td>I usually only do 1 workout a day|I strictly s...</td>\n      <td>1-2 years|</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some adjustments on the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"athletes.csv\", index_col=False, low_memory=False)\n",
    "\n",
    "df['age'].fillna(0, inplace=True)\n",
    "df['height'].fillna(0, inplace=True)\n",
    "df['weight'].fillna(0, inplace=True)\n",
    "\n",
    "df['region'].fillna('', inplace=True)\n",
    "df['gender'].fillna('', inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.628462500Z",
     "start_time": "2024-01-11T20:18:15.059738200Z"
    }
   },
   "id": "6f02b8c8ecab030e",
   "execution_count": 618
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "outputs": [],
   "source": [
    "#Calculates and returns the spans (range of values) for each column in a specified partition of a dataframe, with an option to scale these spans by provided values.\n",
    "def get_spans(df, partition, scale=None):\n",
    "    spans = {}\n",
    "    for feature_column in quasi_identifiers:\n",
    "        if feature_column in categorical:\n",
    "            span = len(df[feature_column][partition].unique())\n",
    "        else:\n",
    "            span = df[feature_column][partition].max() - df[feature_column][partition].min()\n",
    "        if scale is not None:\n",
    "            span = span / scale[feature_column]\n",
    "        spans[feature_column] = span\n",
    "    return spans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.628462500Z",
     "start_time": "2024-01-11T20:18:16.619431Z"
    }
   },
   "id": "6c70412e40d9d7fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Divides a specified partition of a dataframe into two parts based on the median or unique values of a given column, returning a tuple with the indices of these two parts.\n",
    "def split(df, partition, column):\n",
    "    dfp = df[column][partition]\n",
    "    if column in categorical:\n",
    "        values = dfp.unique()\n",
    "        lv = set(values[:len(values) // 2])\n",
    "        rv = set(values[len(values) // 2:])\n",
    "        return dfp.index[dfp.isin(lv)], dfp.index[dfp.isin(rv)]\n",
    "    else:\n",
    "        median = dfp.median()\n",
    "        dfl = dfp.index[dfp < median]\n",
    "        dfr = dfp.index[dfp >= median]\n",
    "        return (dfl, dfr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.629842Z",
     "start_time": "2024-01-11T20:18:16.624691100Z"
    }
   },
   "id": "91f3144e071e27cb",
   "execution_count": 620
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Checks if a partition is k-anonymous by comparing its amount of entries with the required (k).\n",
    "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
    "    if len(partition) < k:\n",
    "        return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.641945300Z",
     "start_time": "2024-01-11T20:18:16.631925800Z"
    }
   },
   "id": "b9a2b33d03f26ba2",
   "execution_count": 621
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Partitions a dataframe into valid subsets based on specified feature columns, a sensitive column, and span scales, using a validity function to ensure each partition meets certain criteria.\n",
    "def partition_dataset(df, feature_columns, sensitive_column, scale, is_valid):\n",
    "    finished_partitions = []\n",
    "    partitions = [df.index]\n",
    "    while partitions:\n",
    "        partition = partitions.pop(0)\n",
    "        spans = get_spans(df[feature_columns], partition, scale)\n",
    "        for column, span in sorted(spans.items(), key=lambda x: -x[1]):\n",
    "            lp, rp = split(df, partition, column)\n",
    "            if not is_valid(df, lp, sensitive_column) or not is_valid(df, rp, sensitive_column):\n",
    "                continue\n",
    "            partitions.extend((lp, rp))\n",
    "            break\n",
    "        else:\n",
    "            finished_partitions.append(partition)\n",
    "    return finished_partitions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.656155800Z",
     "start_time": "2024-01-11T20:18:16.637240300Z"
    }
   },
   "id": "45ea8668dea21938",
   "execution_count": 622
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Aggregates the values of a series with categorical values by concatenating them.\n",
    "def agg_categorical_column(series):\n",
    "\n",
    "    series = set(series.astype(str))\n",
    "    return ','.join(series)\n",
    "    #return [','.join(series)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.657159300Z",
     "start_time": "2024-01-11T20:18:16.643336700Z"
    }
   },
   "id": "d260d0e832101ffd",
   "execution_count": 623
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def agg_numerical_column(series):\n",
    "    return series.mean()\n",
    "    #return [series.mean()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.704615600Z",
     "start_time": "2024-01-11T20:18:16.647624100Z"
    }
   },
   "id": "1f48f0e16c8b2940",
   "execution_count": 624
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Constructs an anonymized dataset by aggregating feature columns and sensitive columns separately for each partition.\n",
    "def build_anonymized_dataset(df, partitions, feature_columns, sensitive_columns,\n",
    "                                                  max_partitions=None):\n",
    "    aggregations = {}\n",
    "    for column in feature_columns:\n",
    "        if column in categorical:\n",
    "            aggregations[column] = agg_categorical_column\n",
    "        else:\n",
    "            aggregations[column] = agg_numerical_column\n",
    "    rows = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        if i % 100 == 1:\n",
    "            print(\"Finished {} partitions...\".format(i))\n",
    "        if max_partitions is not None and i > max_partitions:\n",
    "            break\n",
    "        grouped_columns = df.loc[partition].agg(aggregations, squeeze=False)\n",
    "        values = grouped_columns.to_dict()\n",
    "        # Iterate through each sensitive column and aggregate counts\n",
    "        for sensitive_column in sensitive_columns:\n",
    "            sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({sensitive_column: 'count'})\n",
    "            for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
    "                if count == 0:\n",
    "                    continue\n",
    "                sensitive_values = values.copy()\n",
    "                sensitive_values.update({\n",
    "                    sensitive_column: sensitive_value,\n",
    "                    'count': count,\n",
    "                })\n",
    "                rows.append(sensitive_values)\n",
    "    return pd.DataFrame(rows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.705650400Z",
     "start_time": "2024-01-11T20:18:16.656155800Z"
    }
   },
   "id": "c4cf30a6fb09d064",
   "execution_count": 625
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#categorical attributes that need to be treated in another way than numerical\n",
    "categorical = {'schedule', 'howlong', 'region', 'gender', 'eat'}\n",
    "for name in categorical:\n",
    "    df[name] = df[name].astype('category')\n",
    "    \n",
    "#Our quasi identifiers that we identified earlier\n",
    "quasi_identifiers = ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong']\n",
    "\n",
    "# sensitive-values that should be taken into account\n",
    "sensitive_columns = ['athlete_id', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift',\n",
    "                     'backsq', 'pullups', 'retrieved_datetime']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.750707300Z",
     "start_time": "2024-01-11T20:18:16.661989700Z"
    }
   },
   "id": "e92b677b7074e723",
   "execution_count": 626
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[627], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m full_spans \u001B[38;5;241m=\u001B[39m \u001B[43mget_spans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[619], line 15\u001B[0m, in \u001B[0;36mget_spans\u001B[1;34m(df, partition, scale)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m feature_column \u001B[38;5;129;01min\u001B[39;00m quasi_identifiers:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m feature_column \u001B[38;5;129;01min\u001B[39;00m categorical:\n\u001B[1;32m---> 15\u001B[0m         span \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature_column\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpartition\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m         span \u001B[38;5;241m=\u001B[39m df[feature_column][partition]\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m-\u001B[39m df[feature_column][partition]\u001B[38;5;241m.\u001B[39mmin()\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\series.py:1072\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1069\u001B[0m     key \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(key, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m)\n\u001B[0;32m   1070\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_rows_with_mask(key)\n\u001B[1;32m-> 1072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_with\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\series.py:1099\u001B[0m, in \u001B[0;36mSeries._get_with\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minteger\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1096\u001B[0m     \u001B[38;5;66;03m# We need to decide whether to treat this as a positional indexer\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m     \u001B[38;5;66;03m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001B[39;00m\n\u001B[0;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_should_fallback_to_positional:\n\u001B[1;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1101\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1102\u001B[0m             \u001B[38;5;66;03m# GH#50617\u001B[39;00m\n\u001B[0;32m   1103\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSeries.__getitem__ treating keys as positions is deprecated. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1108\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m   1109\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1150\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1152\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m-> 1153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1382\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1379\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1380\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index with multidimensional key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_iterable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n\u001B[0;32m   1385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_nested_tuple(key, labels):\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1322\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_iterable\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_key(key, axis)\n\u001B[0;32m   1321\u001B[0m \u001B[38;5;66;03m# A collection of keys\u001B[39;00m\n\u001B[1;32m-> 1322\u001B[0m keyarr, indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_listlike_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_reindex_with_indexers(\n\u001B[0;32m   1324\u001B[0m     {axis: [keyarr, indexer]}, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, allow_dups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1325\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1520\u001B[0m, in \u001B[0;36m_LocIndexer._get_listlike_indexer\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1517\u001B[0m ax \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_axis(axis)\n\u001B[0;32m   1518\u001B[0m axis_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_axis_name(axis)\n\u001B[1;32m-> 1520\u001B[0m keyarr, indexer \u001B[38;5;241m=\u001B[39m \u001B[43max\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keyarr, indexer\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6116\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6112\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m   6114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001B[1;32m-> 6116\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6118\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n\u001B[0;32m   6119\u001B[0m     keyarr\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[1;32m~\\PycharmProjects\\PPOD\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:1140\u001B[0m, in \u001B[0;36mRangeIndex.take\u001B[1;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001B[0m\n\u001B[0;32m   1136\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ind_min \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[0;32m   1138\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mind_min\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is out of bounds for axis 0 with size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1139\u001B[0m     )\n\u001B[1;32m-> 1140\u001B[0m taken \u001B[38;5;241m=\u001B[39m \u001B[43mindices\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msafe\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ind_min \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1142\u001B[0m     taken \u001B[38;5;241m%\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "full_spans = get_spans(df, df.index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.933898700Z",
     "start_time": "2024-01-11T20:18:16.752195Z"
    }
   },
   "id": "5190fc885bf63bf1",
   "execution_count": 627
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "finished_partitions = partition_dataset(df, quasi_identifiers, sensitive_columns, full_spans, is_k_anonymous)\n",
    "len(finished_partitions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:18:16.943089600Z",
     "start_time": "2024-01-11T20:18:16.934927700Z"
    }
   },
   "id": "3904e62d591e3ec3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dfn = build_anonymized_dataset(df, finished_partitions, quasi_identifiers, sensitive_columns)\n",
    "# we sort the resulting dataframe using the feature columns and the sensitive attribute\n",
    "#dfn.sort_values()\n",
    "\n",
    "dfn.to_csv(\"k3_anon.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-11T20:18:16.935934200Z"
    }
   },
   "id": "9b0bd1ba37772d37",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3adee71c0da5d1d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extending the k-diverse dataset to also be l-diverse\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfd5955484476ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def diversity(df, partition, column):\n",
    "    print(f\"Partition indices: {partition}\")\n",
    "    \n",
    "    data = df.loc[partition, column].squeeze()\n",
    "    #print(f\"Partition: {partition}\")\n",
    "    #print(f\"Data: {data}\")\n",
    "    unique_values = data.unique() if isinstance(data, pd.Series) else [data]\n",
    "    #print(f\"Unique values: {unique_values}\")\n",
    "    return len(unique_values)\n",
    "\n",
    "def is_l_diverse(df, partition, sensitive_column, l=2):\n",
    "    return diversity(df, partition, sensitive_column) >= l"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T20:32:49.853285200Z",
     "start_time": "2024-01-11T20:32:49.840979600Z"
    }
   },
   "id": "8214935225e196ef",
   "execution_count": 650
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition indices: Index([     2,      3,      7,     11,     14,     19,     24,     26,     28,\n",
      "           29,\n",
      "       ...\n",
      "       422994, 422995, 422996, 422998, 422999, 423001, 423002, 423003, 423004,\n",
      "       423005],\n",
      "      dtype='int64', length=209429)\n",
      "Partition indices: Index([     0,      2,      6,      7,     11,     15,     17,     19,     22,\n",
      "           24,\n",
      "       ...\n",
      "       422974, 422978, 422989, 422993, 422997, 422999, 423000, 423002, 423004,\n",
      "       423005],\n",
      "      dtype='int64', length=204831)\n",
      "Partition indices: Index([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n",
      "            9,\n",
      "       ...\n",
      "       422895, 422896, 422898, 422900, 422902, 422904, 422906, 422908, 422910,\n",
      "       422912],\n",
      "      dtype='int64', length=286822)\n",
      "Partition indices: Index([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n",
      "            9,\n",
      "       ...\n",
      "       422984, 422987, 422988, 422990, 422991, 422994, 422996, 422997, 422999,\n",
      "       423004],\n",
      "      dtype='int64', length=310106)\n",
      "Partition indices: Index([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n",
      "            9,\n",
      "       ...\n",
      "       422996, 422997, 422998, 422999, 423000, 423001, 423002, 423003, 423004,\n",
      "       423005],\n",
      "      dtype='int64', length=422749)\n",
      "Partition indices: Index([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n",
      "            9,\n",
      "       ...\n",
      "       422996, 422997, 422998, 422999, 423000, 423001, 423002, 423003, 423004,\n",
      "       423005],\n",
      "      dtype='int64', length=422664)\n",
      "Partition indices: Index([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n",
      "            9,\n",
      "       ...\n",
      "       422996, 422997, 422998, 422999, 423000, 423001, 423002, 423003, 423004,\n",
      "       423005],\n",
      "      dtype='int64', length=422606)\n"
     ]
    }
   ],
   "source": [
    "df_ldiverse = pd.read_csv(\"k3_anon.csv\", index_col=False, low_memory=False)\n",
    "full_spans = get_spans(df_ldiverse, df_ldiverse.index)\n",
    "\n",
    "finished_l_diverse_partitions = partition_dataset(\n",
    "    df, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))\n",
    "\n",
    "dfldiverse_finished = build_anonymized_dataset(df_ldiverse, finished_l_diverse_partitions, quasi_identifiers, sensitive_columns)\n",
    "dfldiverse_finished.to_csv(\"l2_diverse.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T21:53:45.641913400Z",
     "start_time": "2024-01-11T20:32:50.896086600Z"
    }
   },
   "id": "32da8f97ac74169c",
   "execution_count": 651
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extending the k-diverse dataset to also achieve t-closeness"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32954f7c4f93f649"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e927c25a3c6c2d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def t_closeness(df, partition, sensitive_columns, global_freqs, t=0.3, categorical=None):\n",
    "    # Helper function for numerical t-closeness\n",
    "    def t_closeness_numerical(full_data, partition_data):\n",
    "        ks_stat, _ = ks_2samp(full_data, partition_data)\n",
    "        return ks_stat\n",
    "\n",
    "    # Helper function for categorical t-closeness\n",
    "    def t_closeness_categorical(partition_data, global_freqs):\n",
    "        total_count = float(len(partition_data))\n",
    "        d_max = None\n",
    "        group_counts = partition_data.groupby(column)[column].agg('count')\n",
    "        for value, count in group_counts.to_dict().items():\n",
    "            p = count / total_count\n",
    "            d = abs(p - global_freqs[value])\n",
    "            if d_max is None or d > d_max:\n",
    "                d_max = d\n",
    "        return d_max\n",
    "\n",
    "    # Main t-closeness logic\n",
    "    for column in sensitive_columns:\n",
    "        full_data = df[column]\n",
    "        partition_data = df.loc[partition, column]\n",
    "        \n",
    "        if categorical and column in categorical:\n",
    "            distance = t_closeness_categorical(partition_data, global_freqs[column])\n",
    "        else:\n",
    "            distance = t_closeness_numerical(full_data, partition_data)\n",
    "\n",
    "        if distance > t:\n",
    "            return False\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03f0e8542d8b261"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_tclose = pd.read_csv(\"k3_anon.csv\", index_col=False, low_memory=False)\n",
    "full_spans = get_spans(df_tclose, df_tclose.index)\n",
    "\n",
    "# Get the global frequencies for the sensitive column\n",
    "global_frequencies = {sensitive_column: {} for sensitive_column in sensitive_columns}\n",
    "total_count = len(df)\n",
    "\n",
    "# Determine frequency for every sensitive attribute\n",
    "for sensitive_column in sensitive_columns:\n",
    "    group_counts = df_tclose.groupby(sensitive_column, observed=False)[sensitive_column].agg('count')\n",
    "    for value, count in group_counts.to_dict().items():\n",
    "        p = count / total_count\n",
    "        global_frequencies[sensitive_column][value] = p\n",
    "\n",
    "finished_l_closepartitions = partition_dataset(\n",
    "    df_tclose, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))\n",
    "\n",
    "dfclose_finished = build_anonymized_dataset(df_tclose, finished_l_closepartitions, quasi_identifiers, sensitive_columns)\n",
    "dfclose_finished.to_csv(\"t_close.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2790869de5d00a83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
