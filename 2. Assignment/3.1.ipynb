{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment #2-3: Anonymisation\n",
    "- Dataset: Crossfit [Daset](https://data.world/bgadoci/crossfit-data) (In this assignment only the athletes file was used) \n",
    "- Credits: Dataset was put together by Sam Swift\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read csv as dataframe\n",
    "df = pd.read_csv(\"athletes.csv\", low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:54.653965Z",
     "start_time": "2024-01-10T10:18:53.482503Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Step: Revisit the data set to remind ourselves what we are working with\n",
    "- For a better understanding of the structure of the dataset , we display the attribute values\n",
    "    - What columns does the dataset contain and in what format are the attribute values?\n",
    "        - Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 'athlete_id', Example Data: 2554.0\n",
      "Column: 'name', Example Data: Pj Ablang\n",
      "Column: 'region', Example Data: South West\n",
      "Column: 'team', Example Data: Double Edge\n",
      "Column: 'affiliate', Example Data: Double Edge CrossFit\n",
      "Column: 'gender', Example Data: Male\n",
      "Column: 'age', Example Data: 24.0\n",
      "Column: 'height', Example Data: 70.0\n",
      "Column: 'weight', Example Data: 166.0\n",
      "Column: 'fran', Example Data: 211.0\n",
      "Column: 'helen', Example Data: 645.0\n",
      "Column: 'grace', Example Data: 300.0\n",
      "Column: 'filthy50', Example Data: 1053.0\n",
      "Column: 'fgonebad', Example Data: 0.0\n",
      "Column: 'run400', Example Data: 61.0\n",
      "Column: 'run5k', Example Data: 1081.0\n",
      "Column: 'candj', Example Data: 220.0\n",
      "Column: 'snatch', Example Data: 200.0\n",
      "Column: 'deadlift', Example Data: 400.0\n",
      "Column: 'backsq', Example Data: 305.0\n",
      "Column: 'pullups', Example Data: 25.0\n",
      "Column: 'eat', Example Data: I eat 1-3 full cheat meals per week|\n",
      "Column: 'train', Example Data: I workout mostly at a CrossFit Affiliate|I have a coach who determines my programming|I record my workouts|\n",
      "Column: 'background', Example Data: I played youth or high school level sports|I regularly play recreational sports|\n",
      "Column: 'experience', Example Data: I began CrossFit with a coach (e.g. at an affiliate)|I have attended one or more specialty courses|I have had a life changing experience due to CrossFit|\n",
      "Column: 'schedule', Example Data: I do multiple workouts in a day 2x a week|\n",
      "Column: 'howlong', Example Data: 4+ years|\n",
      "Column: 'retrieved_datetime', Example Data: 2015-03-01 22:49:18\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_not_empty_value(df_column):\n",
    "    return df_column.dropna().iloc[0] if not df_column.dropna().empty else None\n",
    "\n",
    "# Iterate each column \n",
    "for column in df.columns:\n",
    "    first_value = get_first_not_not_empty_value(df[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:54.933266Z",
     "start_time": "2024-01-10T10:18:54.657061Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Anonymisation: Bare Bones – 10 marks\n",
    "Goals: The goal of k-anonymity is to modify a dataset such that any given record cannot be distinguished from at least k−1 other records regarding certain \"quasi-identifier\" attributes. \n",
    "\n",
    "### Algorithm Steps: \n",
    "1. Identify the direct identifier attributes in the data set.\n",
    "2. Identify the quasi-identifiers attributes in the dataset.\n",
    "3. Apply k-anonymity: Choose a value for k (size of the groups of indistinguishable records)\n",
    "4. Use Generalization and Suppression as the transformation methods to transform quasi-identifiers\n",
    "5. Ensure that there are at least k records for each combination of quasi-identifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Identify  direct identifier attributes\n",
    "- By inspecting the different columns and the data format, several attributes which have the potential to contain explicit personally identifiable information can be identified\n",
    "    - `athelete_id`\n",
    "        -  This really depends on the usage of this id! Considerations to take into account are: \n",
    "            - Is the `athlete_id` only used as an internal id of this dataset or does it maybe even refer to an official id?\n",
    "            - Are there other datasets available which may have a similar source to this dataset? Thus, these other datasets may use the same `athlete_id`\n",
    "    - `name`\n",
    "        - The name allows to identify an individual\n",
    "    - `team`\n",
    "        - Depending on the size of the team, this could allow to identify a specific athlete\n",
    "    - `affiliate` \n",
    "        - Depending on the affiliate and the amount of contracted athletes, this could allow to identify an individual\n",
    "    - All stats of the athletes\n",
    "        - If an athlete has really remarkable stats (maybe even a world record in a category), this could allow to identify the individual\n",
    "    - `train` \n",
    "        - If an athlete has a special and famous training routine, this could allow to identify him\n",
    "    - `background`\n",
    "        - If an athlete has a famous background or mentions names, this could allow to identify him\n",
    "    - `experience`\n",
    "        - If an athlete mentions concrete information about his experience (e.g. name of current coach), this could allow to identify him\n",
    "\n",
    "-> As can be seen, all columns could potentially contain outliers which could be then used to identify an individual. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Identify the quasi-identifiers attributes in the dataset\n",
    "- In this step, we use the following script to search for any attributes qualifying as a quasi-identifiers not flagged as PII in the step before. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Quasi-Identifiers: ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong', 'retrieved_datetime']\n"
     ]
    }
   ],
   "source": [
    "# identify potential quasi-identifiers\n",
    "def identify_quasi_identifiers(dataframe, sensitive_columns):\n",
    "    quasi_identifiers = []\n",
    "    for column in dataframe.columns:\n",
    "        # Skip sensitive attributes\n",
    "        if column in sensitive_columns:\n",
    "            continue\n",
    "        \n",
    "        unique_count = dataframe[column].nunique()\n",
    "        # Assume a column could be a quasi-identifier if it's not unique for each record\n",
    "        # but has a high number of unique values.\n",
    "        if 1 < unique_count < len(dataframe):\n",
    "            quasi_identifiers.append(column)\n",
    "    \n",
    "    return quasi_identifiers\n",
    "\n",
    "# column names we know are PII\n",
    "sensitive_columns = ['athlete_id', 'name', 'team', 'affiliate', 'train', 'background', 'experience', 'fran', 'helen',  'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq', 'pullups']\n",
    "\n",
    "# Identify potential quasi-identifiers\n",
    "potential_quasi_identifiers = identify_quasi_identifiers(df, sensitive_columns)\n",
    "print(\"Potential Quasi-Identifiers:\", potential_quasi_identifiers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:54.985861Z",
     "start_time": "2024-01-10T10:18:54.942466Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Apply k-anonymity: Choose a value for k\n",
    "- In this step we choose a value for k. \n",
    "- For example if we choose k = 3, then each combination of quasi-identifier values should apply to at least three records in the given dataset\n",
    "- a higher k value strengthens privacy by making re-identification more difficult, it also reduces the utility of the data by increasing information loss. \n",
    "- The choice of k thus represents a trade-off between privacy and utility that must be considered in the context of how the data will be used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Use Generalization and Suppression as the transformation methods\n",
    "- Some data exploration has been done in #Assignment 1 already, so the intervals for different attributes and replacement values can be recycled. Some exploration must be done on top of it.\n",
    "- The attribute 'retrieved_datetime' can be removed, since all the entries are empty. Also, we standardize empty values.\n",
    "- The quasi-identifiers 'age', 'height', 'weight' will be anonymized using aggregation\n",
    "- Every attribute occuring less than 50 times in the entire dataset will be suppressed\n",
    "- The 'regions' will be mapped to the 7 continents\n",
    "- "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "#drop the 'retrieved_datetime' column\n",
    "df = df.drop(columns=['retrieved_datetime'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.042417Z",
     "start_time": "2024-01-10T10:18:54.986139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "# Iterate over all columns in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Replace empty strings with 'NA' in the column\n",
    "    df[column] = df[column].replace({'': 'NA'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.284147Z",
     "start_time": "2024-01-10T10:18:55.043323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "# Aggregate age, height and weight\n",
    "bins_age = [0, 18, 30, 45, 60, 100]\n",
    "labels_age = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "bins_height = [0, 20, 40, 60, 70, 80, 90]\n",
    "labels_height = ['0-20', '20-40', '40-60', '60-70', '70-80', '81+']\n",
    "\n",
    "bins_weight = [0, 159, 169, 179, 189, 199, 220]\n",
    "labels_weight = ['0-159', '160-169', '170-179', '180-189', '190-199', '200+']\n",
    "\n",
    "# Apply binning\n",
    "df['age'] = pd.cut(df['age'], bins=bins_age, labels=labels_age)\n",
    "df['height'] = pd.cut(df['height'], bins=bins_height, labels=labels_height)\n",
    "df['weight'] = pd.cut(df['weight'], bins=bins_weight, labels=labels_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.318669Z",
     "start_time": "2024-01-10T10:18:55.285965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "# List of columns to apply the suppression\n",
    "columns_to_suppress = ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong']\n",
    "\n",
    "for column in columns_to_suppress:\n",
    "    # Counting the frequency of each unique value in the column\n",
    "    value_counts = df[column].value_counts()\n",
    "\n",
    "    # Identifying values that occur less than 20 times\n",
    "    values_to_remove = value_counts[value_counts < 100].index\n",
    "\n",
    "    # Removing rows with these values\n",
    "    df = df[~df[column].isin(values_to_remove)]\n",
    "\n",
    "# After this loop, df contains your DataFrame with rare values removed in each specified column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.798959Z",
     "start_time": "2024-01-10T10:18:55.328357Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique regions: ['South West' nan 'Southern California' 'South Central' 'Central East'\n",
      " 'Europe' 'North East' 'Africa' 'South East' 'Australia'\n",
      " 'Northern California' 'Latin America' 'Canada East' 'North Central'\n",
      " 'North West' 'Mid Atlantic' 'Canada West' 'Asia']\n"
     ]
    }
   ],
   "source": [
    "#Data exploration to map the regions in a meaningful way\n",
    "unique_regions = df['region'].unique()\n",
    "print(\"Unique regions:\", unique_regions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.806300Z",
     "start_time": "2024-01-10T10:18:55.803643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [],
   "source": [
    "# Example mapping of regions to continents, including 'NA'\n",
    "region_to_continent = {\n",
    "    'South West': 'North America',\n",
    "    'Southern California': 'North America',\n",
    "    'South Central': 'North America',\n",
    "    'Central East': 'North America',\n",
    "    'Europe': 'Europe',\n",
    "    'North East': 'North America',\n",
    "    'Africa': 'Africa',\n",
    "    'South East': 'North America',\n",
    "    'Australia': 'Oceania',\n",
    "    'Northern California': 'North America',\n",
    "    'Latin America': 'South America',\n",
    "    'Canada East': 'North America',\n",
    "    'North Central': 'North America',\n",
    "    'North West': 'North America',\n",
    "    'Mid Atlantic': 'North America',\n",
    "    'Canada West': 'North America',\n",
    "    'Asia': 'Asia',\n",
    "    'NA': 'NA'  # Preserving 'NA' as is\n",
    "}\n",
    "# Apply the mapping to the 'region' column\n",
    "df['region'] = df['region'].map(region_to_continent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.839019Z",
     "start_time": "2024-01-10T10:18:55.806018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Ensure that there are at least k records for each combination of quasi-identifiers\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 420955\n",
      "Unique values in each column:\n",
      " region       6\n",
      "gender       2\n",
      "age          4\n",
      "height       4\n",
      "weight       6\n",
      "eat         19\n",
      "schedule    40\n",
      "howlong     10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check how many unique values there are in each column\n",
    "num_rows = len(df)\n",
    "print(\"Number of rows:\", num_rows)\n",
    "\n",
    "columns_of_interest = ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong']\n",
    "selected_df = df[columns_of_interest]\n",
    "\n",
    "unique_values = selected_df.nunique()\n",
    "print(\"Unique values in each column:\\n\", unique_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.889578Z",
     "start_time": "2024-01-10T10:18:55.841288Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset does NOT satisfy k=2 anonymity.\n",
      "Groups that occur only once:\n",
      "               region  gender    age height   weight  \\\n",
      "0             Africa  Female   0-18  60-70    0-159   \n",
      "1             Africa  Female   0-18  60-70    0-159   \n",
      "2             Africa  Female  19-30  40-60    0-159   \n",
      "3             Africa  Female  19-30  40-60    0-159   \n",
      "4             Africa  Female  19-30  60-70    0-159   \n",
      "...              ...     ...    ...    ...      ...   \n",
      "22926  South America    Male  46-60  70-80  180-189   \n",
      "22927  South America    Male  46-60  70-80  190-199   \n",
      "22928  South America    Male  46-60  70-80  190-199   \n",
      "22929  South America    Male  46-60  70-80     200+   \n",
      "22930  South America    Male  46-60  70-80     200+   \n",
      "\n",
      "                                                     eat  \\\n",
      "0      I eat quality foods but don't measure the amount|   \n",
      "1                           I weigh and measure my food|   \n",
      "2                   I eat 1-3 full cheat meals per week|   \n",
      "3      I eat quality foods but don't measure the amou...   \n",
      "4                                     Decline to answer|   \n",
      "...                                                  ...   \n",
      "22926                       I weigh and measure my food|   \n",
      "22927  I eat quality foods but don't measure the amount|   \n",
      "22928  I eat strict Paleo|I eat quality foods but don...   \n",
      "22929  I eat quality foods but don't measure the amount|   \n",
      "22930  I eat quality foods but don't measure the amou...   \n",
      "\n",
      "                                                schedule              howlong  \\\n",
      "0       I do multiple workouts in a day 3+ times a week|           2-4 years|   \n",
      "1                     I usually only do 1 workout a day|           1-2 years|   \n",
      "2      I usually only do 1 workout a day|I strictly s...           1-2 years|   \n",
      "3                     I usually only do 1 workout a day|         6-12 months|   \n",
      "4       I do multiple workouts in a day 3+ times a week|         6-12 months|   \n",
      "...                                                  ...                  ...   \n",
      "22926  I do multiple workouts in a day 3+ times a wee...           2-4 years|   \n",
      "22927  I do multiple workouts in a day 3+ times a wee...           1-2 years|   \n",
      "22928                 I usually only do 1 workout a day|           1-2 years|   \n",
      "22929  I do multiple workouts in a day 3+ times a wee...  Less than 6 months|   \n",
      "22930   I do multiple workouts in a day 3+ times a week|         6-12 months|   \n",
      "\n",
      "       count  \n",
      "0          1  \n",
      "1          1  \n",
      "2          1  \n",
      "3          1  \n",
      "4          1  \n",
      "...      ...  \n",
      "22926      1  \n",
      "22927      1  \n",
      "22928      1  \n",
      "22929      1  \n",
      "22930      1  \n",
      "\n",
      "[16638 rows x 9 columns]\n",
      "Number of groups not satisfying k=2 anonymity: 16638\n"
     ]
    }
   ],
   "source": [
    "# Group by quasi-identifiers\n",
    "grouped_df = df.groupby(['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong'], observed=True).size().reset_index(name='count')\n",
    "\n",
    "# Check the minimum count\n",
    "min_count = grouped_df['count'].min()\n",
    "non_compliant_groups = grouped_df[grouped_df['count'] < 2]\n",
    "# Verify if k-anonymity is achieved\n",
    "if min_count >= 2:\n",
    "    print(\"The dataset satisfies k=2 anonymity.\")\n",
    "else:\n",
    "    print(\"The dataset does NOT satisfy k=2 anonymity.\")\n",
    "\n",
    "    # Display groups that occur only once\n",
    "    only_once = grouped_df[grouped_df['count'] == 1]\n",
    "    print(\"Groups that occur only once:\\n\", only_once)\n",
    "    \n",
    "    num_non_compliant = len(non_compliant_groups)\n",
    "    print(\"Number of groups not satisfying k=2 anonymity:\", num_non_compliant)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T10:18:55.973426Z",
     "start_time": "2024-01-10T10:18:55.890332Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
